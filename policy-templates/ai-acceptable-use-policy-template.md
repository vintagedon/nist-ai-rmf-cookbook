# AI Acceptable Use Policy

**Document ID:** [Your document control ID - e.g., POL-AI-001]  
**Version:** [x.x]  
**Effective Date:** [YYYY-MM-DD]  
**Policy Owner:** [Role - e.g., Chief Technology Officer, AI Risk Officer]  
**Review Cycle:** [Annual, Semi-Annual, etc.]  
**Scope:** [Who this applies to - e.g., All personnel, contractors, and AI agents]

[GUIDANCE: This policy defines HOW your organization uses AI responsibly. It's the operational rulebook that translates governance principles into daily practice. The policy should answer: What AI tools can we use? With what data? For what purposes? What are the boundaries?]

---

> **RadioAstronomy.io Implementation:**
>
> **Document ID:** POL-AI-001  
> **Version:** 3.0  
> **Effective Date:** 2025-01-27  
> **Policy Owner:** Chief Technology Officer  
> **Review Cycle:** Annual  
> **Scope:** All personnel, contractors, and AI agents within RadioAstronomy.io
>
> ---
>
> **About RadioAstronomy.io:**
>
> RadioAstronomy.io is a six-person citizen science organization operating a Proxmox-based astronomy compute cluster. We publish astronomical datasets to Zenodo, contribute research to Value-Added Catalogs (VACs), maintain open-source astronomy tools on GitHub, and donate compute time to astronomy students worldwide. Our work appears in scientific literature, our datasets get cited in research papers, and our infrastructure decisions affect student researchers.
>
> **Why This Policy Exists:**
>
> We operate as an **AI-native organization**—AI agents are part of our operational workforce, not just tools we occasionally use. This creates responsibility: our AI-assisted work has downstream impact on scientific research globally. This policy establishes clear guidelines ensuring we use AI responsibly while maintaining the productivity benefits that make AI integration valuable.
>
> **Voluntary High-Risk Posture:**
>
> We voluntarily adopt high-risk AI deployment practices even though our organizational size doesn't legally require it. Our downstream impact exceeds our organizational footprint—published datasets get cited, errors propagate through research, automated decisions affect student access. We follow Colorado SB24-205 requirements voluntarily (impact assessments, transparency, algorithmic management standards), operate within CIS Controls v8.1 IG1 for cybersecurity, align with NIST AI RMF 1.0 for risk management, and follow ISO 31000:2018 for risk practices.
>
> The frameworks aren't the goal—they're the scaffolding that keeps high-impact work disciplined.

---

## 1. Introduction

### 1.1 About [Your Organization]

[GUIDANCE: Start with organizational context. Who are you? What do you do? Why does AI matter to your mission? This grounds the policy in operational reality rather than abstract compliance.]

[Your organization] is a [organizational description]. We [what you do] for [who you serve]. Our work [impact statement - who depends on your outputs, what happens if you fail, why quality matters].

### 1.2 AI-Native Operations

[GUIDANCE: Describe your relationship with AI. Is AI "tools we use sometimes" or "embedded in how we work"? This sets expectations and explains policy scope.]

We [describe AI integration level - e.g., "use AI tools occasionally for specific tasks", "operate AI-native with agents embedded throughout workflows", "are experimenting with AI adoption", "have strategic AI initiatives"]. 

[If you use AI extensively, describe your multi-agent architecture or AI operational model. If not, describe current usage patterns and planned evolution.]

### 1.3 [Compliance Posture / Risk Stance / Governance Approach]

[GUIDANCE: Explain WHY you have rigorous AI governance. Options include:
- Regulatory compliance (you're required to follow specific laws/standards)
- Voluntary high-risk posture (downstream impact justifies extra rigor)
- Industry best practice (competitive advantage, customer expectations, stakeholder demands)
- Risk management (protecting critical assets, preventing harm, maintaining reputation)]

We [explain your governance rationale]. [If following specific frameworks, name them and explain why you chose those particular frameworks versus alternatives.]

**Alternative Frameworks That Can Drop in Module-Style:**

- **Regulatory Compliance:** If operating in Colorado with consequential AI systems, Colorado SB24-205 is mandatory. EU organizations should consider EU AI Act requirements. Federal contractors need FedRAMP compliance. Financial services might need OCC guidance or NIST AI RMF alignment for regulatory examination.

- **Voluntary Best Practice:** NIST AI RMF 1.0 provides comprehensive risk framework suitable for any organization. ISO 42001 offers AI management system structure for organizations preferring ISO standards. CIS Controls v8.1 provides cybersecurity baseline (IG1 for small orgs, IG2/IG3 for larger/higher-risk).

### 1.4 Purpose of This Policy

[GUIDANCE: What does this policy accomplish? Be specific about what behaviors it enables/restricts and why.]

This policy establishes clear guidelines for responsible AI use within [Organization]. It defines:

- [Outcome 1 - e.g., Data classification boundaries determining AI tool selection]
- [Outcome 2 - e.g., Role-based responsibilities for AI usage]
- [Outcome 3 - e.g., Risk management practices aligned with compliance frameworks]
- [Outcome 4 - e.g., Enforcement mechanisms and accountability structures]

This policy applies to [scope - e.g., all operational systems, published work, infrastructure automation, customer interactions]. Compliance is mandatory for [who - e.g., all personnel, contractors, and AI agents].

---

> **RadioAstronomy.io Purpose:**
>
> ### 1.4 Purpose of This Policy
>
> This policy establishes clear guidelines for responsible AI use within RadioAstronomy.io. It defines:
>
> 1. **Provide clear guidance** on which AI models to use for which data classifications and work contexts
> 2. **Ensure confidentiality and protection** of sensitive organizational and research data when interacting with AI technologies
> 3. **Mitigate risks** associated with AI misuse, bias, inaccuracy, and security vulnerabilities
> 4. **Foster awareness** among all personnel about their responsibilities when using AI tools
> 5. **Promote ethical integration** of AI into scientific research and infrastructure operations
> 6. **Enable AI-native operations** while maintaining compliance with voluntary high-risk standards
> 7. **Support citizen science mission** through responsible AI-assisted research and student compute donation
> 8. **Maintain transparency** with the research community about our AI usage in published work
>
> This policy applies to all operational systems, published work, infrastructure automation, and community contributions. Compliance is mandatory for all personnel, contractors, and AI agents.

---

## 2. Policy Objectives

[GUIDANCE: List 5-8 measurable objectives. These should be specific enough to verify compliance but broad enough to remain stable over time.]

The primary objectives of this policy include:

1. [Objective 1 - e.g., Clear tool selection guidance based on data sensitivity and use context]
2. [Objective 2 - e.g., Data protection aligned with organizational classification scheme]
3. [Objective 3 - e.g., Risk mitigation for identified AI-specific threats]
4. [Objective 4 - e.g., User training and competency development]
5. [Objective 5 - e.g., Ethical AI integration supporting organizational mission]
[Continue as needed]

---

> **RadioAstronomy.io Objectives:**
>
> ## 2. Policy Objectives
>
> The primary objectives of this policy include:
>
> 1. **Provide clear guidance** on which AI models to use for which data classifications and work contexts
> 2. **Ensure confidentiality and protection** of sensitive organizational and research data when interacting with AI technologies
> 3. **Mitigate risks** associated with AI misuse, bias, inaccuracy, and security vulnerabilities
> 4. **Foster awareness** among all personnel about their responsibilities when using AI tools
> 5. **Promote ethical integration** of AI into scientific research and infrastructure operations
> 6. **Enable AI-native operations** while maintaining compliance with voluntary high-risk standards
> 7. **Support citizen science mission** through responsible AI-assisted research and student compute donation
> 8. **Maintain transparency** with the research community about our AI usage in published work

---

## 3. Scope

### 3.1 Applicability

[GUIDANCE: Define who and what this policy covers. Be explicit about boundaries.]

This policy applies to:

- [Who - e.g., all employees, contractors, consultants, temporary personnel, AI agents]
- [What activities - e.g., all AI technology usage regardless of deployment method]
- [What systems - e.g., all business units, projects, operational systems, geographic locations]

### 3.2 Coverage

[GUIDANCE: List specific AI technologies and use cases in scope. Prevents "I didn't know that was covered" confusion.]

This policy covers:

- [AI type 1 - e.g., Generative AI services (ChatGPT, Claude, Gemini, etc.)]
- [AI type 2 - e.g., AI-powered productivity tools and coding assistants]
- [AI type 3 - e.g., Autonomous agents and automation systems]
- [AI type 4 - e.g., AI-assisted [your domain-specific applications]]
[Continue as appropriate for your context]

### 3.3 Integration

[GUIDANCE: How does this policy connect to other organizational practices? Shows this isn't isolated—it's part of broader governance.]

This policy integrates into broader organizational practices including:

- [Practice 1 - e.g., Data classification and protection standards]
- [Practice 2 - e.g., Access control and identity management]
- [Practice 3 - e.g., [Domain-specific practice relevant to your work]]
- [Practice 4 - e.g., Staff training and competency development]
- [Practice 5 - e.g., Incident response and risk management]

---

> **RadioAstronomy.io Scope:**
>
> ### 3.1 Applicability
>
> This policy applies to:
>
> - All RadioAstronomy.io employees, contractors, consultants, and temporary personnel
> - All AI agents operating within RadioAstronomy.io infrastructure (Claude instances, Gemini CLI, automation systems)
> - All activities involving AI technologies across operational, research, and community functions
> - All business units, projects, and operational systems regardless of geographic location
>
> ### 3.2 Coverage
>
> This policy covers:
>
> - Generative AI services (Gemini, Claude, local models)
> - AI-powered productivity tools and coding assistants
> - Autonomous automation agents (Gemini CLI operations)
> - AI-assisted infrastructure management and cluster orchestration
> - AI use in research data processing, analysis, and publication
> - AI use in documentation, community contributions, and open-source projects
>
> ### 3.3 Integration
>
> This policy integrates into broader organizational practices including:
>
> - Data classification and protection standards
> - Access control and identity management
> - Research integrity and publication ethics
> - Infrastructure security and operations
> - Staff training and competency development
> - Incident response and risk management

---

## 4. Data Classification Framework

[GUIDANCE: **This is the core organizing principle.** Your data classification defines what data goes where. Every AI decision starts here. Most organizations have 3-5 tiers. Common patterns:
- 3-tier: Public / Internal / Confidential
- 4-tier: Public / Internal / Sensitive / Secret
- 5-tier: Public / Internal / Confidential / Restricted / Top Secret

Choose based on your data sensitivity range and operational complexity. More tiers = more precision but more complexity.]

**Our data classification defines what goes where.** Every interaction with AI starts here.

### 4.X [Highest Sensitivity Tier - Never AI]

[GUIDANCE: Define your most sensitive data that should NEVER touch AI under any circumstances. Common examples: credentials, passwords, API keys, PII, PHI, financial credentials, encryption keys, customer payment data.]

**Definition:** [List specific data types in this category]

**Boundary:** Never input [tier name] into any AI model under any circumstances. Use [alternative systems - e.g., secrets management, encrypted storage, access control] instead.

**Why this matters:** [Explain the risk - e.g., credential exposure creates immediate security compromise, regulatory violation, customer harm]

**Applicable Controls:** [Reference your technical controls - e.g., CIS Controls, ISO 27001 controls, internal security standards]

### 4.X [Next Sensitivity Tier]

[GUIDANCE: Define your next tier. Common pattern: data that needs protection but can use AI with appropriate controls. Examples: draft documents, internal strategies, work-in-progress, proprietary algorithms, employee data.]

**Definition:** [List specific data types]

**Boundary:** [Describe allowed AI usage - e.g., "local models only", "approved enterprise services with no-training guarantees", "executive-isolated workspace"]

**Why this matters:** [Explain what you're protecting and why the boundary makes sense]

**Applicable Controls:** [Reference controls that enable safe use]

[Continue pattern for each tier]

### 4.X [Lowest Sensitivity Tier - Public]

[GUIDANCE: Define data already public or intended for public release. This tier usually has fewest restrictions on AI usage.]

**Definition:** [List specific data types]

**Boundary:** [Describe allowed AI usage - broadest permissions tier]

**Why this matters:** [Explain why broader usage acceptable for this tier]

**Applicable Controls:** [Even public data may need some controls - e.g., attribution, quality review, bias checking]

**Alternative Classification Schemes That Can Drop in Module-Style:**

- **Government/Defense:** Use FIPS 199 categories (Low/Moderate/High impact) or classification levels (Unclassified/Confidential/Secret/Top Secret) depending on your sector
- **Healthcare:** Use HIPAA categories (PHI/De-identified/Public) aligned with regulatory requirements
- **Financial Services:** Use regulatory categories (NPI/Customer Data/Public) matching OCC or FINRA expectations
- **Standard Corporate:** Use typical 3-tier (Public/Internal/Confidential) if simpler needs

---

> **RadioAstronomy.io Data Classification:**
>
> ## 4. Data Classification Framework
>
> **Our data classification defines what goes where.** Every interaction with AI starts here. This is the core organizing principle that determines which AI models are appropriate for which work.
>
> ### 4.1 Secrets (Never AI)
>
> **Definition:** Credentials, API keys, tokens, passwords, personally identifiable information (PII), protected health information (PHI), SSH keys, TLS certificates, encryption keys, service account credentials.
>
> **Boundary:** Never input Secrets into any AI model under any circumstances. Use secrets management systems (HashiCorp Vault, GCP Secret Manager) and credential rotation instead.
>
> **Why this matters:** Secrets exposure creates immediate security compromise. AI models—even with enterprise agreements—introduce unnecessary risk surfaces for credential handling. Architectural certainty requires zero AI contact with authentication materials.
>
> **Applicable Controls:**
>
> - CIS 3.11 (Data Protection) - Encrypt sensitive data at rest
> - CIS 6.1 (Access Control) - Centralized secrets management
> - NIST AI RMF MAP 1.1 - Context of use includes sensitive data handling
>
> ### 4.2 Protected (Local or CTO-Isolated Only)
>
> **Definition:** Draft research before publication, proprietary algorithms, student research data before release, internal security assessments, pre-decision governance drafts, configuration files containing implementation details.
>
> **Boundary:** Process on local models (Llama-3-70B on A4000 GPU) or via CTO-isolated Claude workspace. Cloud models (Gemini, Z.ai) cannot access Protected data.
>
> **Why this matters:** Protected data represents work-in-progress with strategic or competitive sensitivity. Local processing provides architectural air-gap. CTO isolation ensures appropriate oversight for high-stakes review.
>
> **Applicable Controls:**
>
> - CIS 3.12 (Data Protection) - Segment data processing by classification
> - NIST AI RMF GOVERN 1.6 - Policies for third-party risk (cloud model vendors)
> - ISO 31000 Clause 6.4 - Risk treatment via architectural isolation
>
> ### 4.3 Public-Internal (Gemini/Claude, Training Opt-Out)
>
> **Definition:** Our internal code repositories, documentation, runbooks, published research/datasets (post-release), operational logs, architecture diagrams, internal processes.
>
> **Boundary:** Use Gemini Pro 2.5 (enterprise SSO, training opt-out enforced) or Claude (CTO for high-stakes work). This is "our IP, but not secret"—appropriate for enterprise models with data protection agreements.
>
> **Why this matters:** Public-Internal represents organizational knowledge we control and may eventually publish, but isn't public yet. Enterprise models with training opt-out provide productivity without IP leakage risk.
>
> **Applicable Controls:**
>
> - CIS 9.2 (Data Protection) - Data loss prevention on enterprise platforms
> - NIST AI RMF GOVERN 1.7 - Transparency in AI operations (training opt-out documented)
> - Colorado SB24-205 §6-1-1703(2)(b) - Transparency regarding training data practices
>
> ### 4.4 Public-External (Z.ai Allowed, Engineer Role Only)
>
> **Definition:** Forked OSS repositories we don't control, community contributions, public astronomy datasets bound for publication with unrestricted global access, upstream pull requests, citizen science code contributions.
>
> **Boundary:** Z.ai GLM-4-Plus is permitted for Public-External astronomy and data science research code only. Engineer role required. Internal repositories (even if public on GitHub) remain Public-Internal (use Gemini/Claude).
>
> **Why this matters:** Research code will be published to GitHub/Zenodo/journals with unrestricted access, making training acceptable. Z.ai accelerates citizen-science contributions. Boundary enforcement prevents internal code exposure.
>
> **Applicable Controls:**
>
> - CIS 6.8 (Access Control) - Role-based access to tools
> - NIST AI RMF GOVERN 5.1 - Organizational policies for third-party AI use
> - ISO 31000 Clause 6.4.2 - Risk treatment via scope boundaries

---

## 5. Role-Based AI Stack

[GUIDANCE: Map organizational roles to approved AI tools and permitted data tiers. This creates "I'm a [role], I work with [data type], therefore I use [these tools]" decision framework. Prevents every user from needing to become AI governance expert.

Common approach: Create 3-7 roles covering your organizational functions, define AI tool access for each based on their typical data handling and use cases.]

**Role-based access ensures right tools for right use cases.** Not everyone needs access to every AI tool.

### 5.X [Role Name]

[GUIDANCE: For each role, define:]

**Typical Responsibilities:** [What this role does day-to-day]

**Data Access:** [Which classification tiers they regularly work with]

**Approved AI Tools:**

- **[Tool Name]:** [When/how to use - with which data types, for what purposes]
  - **Data Classification Boundary:** [Maximum sensitivity allowed]
  - **Use Cases:** [Specific examples]
  - **Restrictions:** [What NOT to do with this tool]

[Repeat for each tool this role can access]

**Rationale:** [Why this combination of tools makes sense for this role's work patterns and data access]

[Repeat section for each role]

**Alternative Stack Organization Patterns:**

- **Flat Access Model:** Everyone gets same tools; rely on training and data classification only (works for <10 person orgs with high trust)
- **Department-Based:** Marketing stack ≠ Engineering stack ≠ Finance stack (works when departments have distinct data handling)
- **Clearance-Based:** Tools tied to data clearance levels, not job functions (works for government/defense)
- **Use-Case-Based:** Tools approved for specific workflows rather than roles (works when roles are fluid but workflows are stable)

---

> **RadioAstronomy.io Role-Based AI Stack:**
>
> ## 5. Role-Based AI Stack
>
> **Role-based access ensures right tools for right use cases.** Not everyone needs access to every AI tool.
>
> [GUIDANCE: RadioAstronomy.io has 6 roles. For template usage, include 3-5 representative roles covering your typical organizational functions: technical, research/analytical, operations, executive, etc. Scale and adapt as needed.]
>
> ### 5.1 Engineer (Research & Infrastructure)
>
> **Typical Responsibilities:** Astronomy research pipelines, data analysis, cluster infrastructure, coding, DevOps, technical documentation
>
> **Data Access:** All tiers (Secrets access for infrastructure, Protected for research, Public-Internal for ops, Public-External for OSS contributions)
>
> **Approved AI Tools:**
>
> - **Gemini Pro 2.5 (Google Workspace Enterprise):**
>   - **Data Classification Boundary:** Public-Internal maximum (no Secrets, no Protected)
>   - **Use Cases:** Code review, documentation drafting, debugging, architecture brainstorming, operational runbooks, internal tool development
>   - **Restrictions:** No Protected research data, no credentials, no student data pre-anonymization
>   - **Why:** Enterprise SSO, training opt-out enforced, DLP enabled, integrated into Google Workspace where most ops/docs work happens
>
> - **Claude Sonnet 4 (Anthropic Enterprise - CEO Workspace):**
>   - **Data Classification Boundary:** Protected allowed (CTO-isolated workspace with CEO oversight)
>   - **Use Cases:** Draft research analysis, proprietary algorithm development, governance policy drafting, security architecture decisions
>   - **Restrictions:** Must be explicitly approved by CTO for Protected data sessions, no Secrets ever
>   - **Why:** No-training guarantee, isolated workspace prevents cross-project leakage, CEO oversight ensures appropriate use of Protected data handling
>
> - **Z.ai GLM-4-Plus:**
>   - **Data Classification Boundary:** Public-External only (OSS code, published datasets, citizen science contributions)
>   - **Use Cases:** Forked OSS repository work, astronomy data science code for publication, pull requests to upstream projects, community tool development
>   - **Restrictions:** NEVER internal repositories (even if public on GitHub), NEVER organizational code/infrastructure, ONLY astronomy/data science domain
>   - **Why:** Training acceptable for public research code; accelerates citizen science contributions; strict boundary prevents internal code exposure
>
> - **Local Llama-3-70B (A4000 GPU, agents01):**
>   - **Data Classification Boundary:** Protected allowed (local processing, no network egress)
>   - **Use Cases:** Protected research data analysis, sensitive algorithm testing, offline work, air-gapped experiments
>   - **Restrictions:** Performance limited vs cloud models; model may lag latest capabilities
>   - **Why:** Architectural air-gap for Protected data; no vendor data handling; full control over model and data
>
> **Rationale:** Engineers work across all data tiers and need flexible tool access. Gemini for daily ops (fast, integrated, Public-Internal boundary). Claude for high-stakes Protected work (CTO oversight). Z.ai for public research (training acceptable, domain-specific). Local models for offline/Protected (air-gapped). Clear boundaries prevent classification confusion.
>
> ### 5.2 Research Lead
>
> **Typical Responsibilities:** Literature review, experiment design, data interpretation, scientific writing, publication preparation, collaboration management
>
> **Data Access:** Protected (draft research), Public-Internal (published datasets), Public-External (literature, OSS tools)
>
> **Approved AI Tools:**
>
> - **Gemini Pro 2.5:**
>   - **Data Classification Boundary:** Public-Internal (published research, literature summaries, final datasets)
>   - **Use Cases:** Literature review summarization, final paper editing, published dataset documentation, email to collaborators
>   - **Restrictions:** No draft research, no unpublished findings, no pre-publication data analysis
>
> - **Claude Sonnet 4 (CEO Workspace):**
>   - **Data Classification Boundary:** Protected (draft research with CTO approval)
>   - **Use Cases:** Early-stage research analysis, hypothesis development, draft paper writing, sensitive collaboration discussions
>   - **Restrictions:** CTO must approve Protected data sessions; no Secrets; outputs reviewed before publication
>
> - **Local Llama-3-70B:**
>   - **Data Classification Boundary:** Protected (local-only processing)
>   - **Use Cases:** Exploratory data analysis before publication decisions, algorithm prototyping, offline research review
>   - **Restrictions:** Model limitations may affect output quality vs cloud models
>
> **Rationale:** Research Leads work primarily with Protected (pre-publication) and Public-Internal (post-publication) data. Don't need Z.ai (not doing OSS contributions). Need Claude access for Protected draft research with oversight. Gemini for post-publication work. Local models for offline exploration.
>
> ### 5.3 Operations Automation (Gemini CLI Agent)
>
> **Typical Responsibilities:** Scheduled tasks, monitoring, reporting, infrastructure health checks, automated documentation, log analysis
>
> **Data Access:** Public-Internal (logs, metrics, configs), limited Protected (student anonymized data for reporting)
>
> **Approved AI Tools:**
>
> - **Gemini Pro 2.5 via CLI:**
>   - **Data Classification Boundary:** Public-Internal maximum
>   - **Use Cases:** Nightly cluster health reports, log summarization, automated documentation updates, metric analysis, alert triaging
>   - **Restrictions:** No Secrets (credentials managed via Vault), no Protected student PII (only anonymized aggregates), no manual intervention—fully automated
>   - **Why:** CLI integration enables automation, enterprise SSO, training opt-out, operates within compliance fence (Google Workspace Enterprise)
>
> **Rationale:** Automation agent needs programmatic access (Gemini CLI), operates unattended (no human review per task), limited to Public-Internal data to prevent autonomous Protected data handling without oversight. Single-tool stack simplifies automation architecture.
>
> ### 5.4 CTO / Executive
>
> **Typical Responsibilities:** Strategic direction, governance oversight, high-stakes decisions, Protected data stewardship, policy approval, risk acceptance
>
> **Data Access:** All tiers (appropriate for oversight role)
>
> **Approved AI Tools:**
>
> - **Claude Sonnet 4 (Personal Workspace):**
>   - **Data Classification Boundary:** Protected allowed (personal isolated workspace)
>   - **Use Cases:** Governance policy development, risk assessment review, strategic planning, sensitive decision analysis, Protected research oversight
>   - **Restrictions:** No Secrets; Protected data handling documented; outputs involving Protected data reviewed before wider distribution
>   - **Why:** No-training guarantee, personal workspace isolation, appropriate for executive handling of sensitive strategic/governance content
>
> - **Gemini Pro 2.5:**
>   - **Data Classification Boundary:** Public-Internal
>   - **Use Cases:** Operational decisions, team communication, Public-Internal documentation, published research review
>   - **Restrictions:** No Protected (use Claude), no Secrets
>
> **Rationale:** CTO needs highest-tier access for governance and oversight. Claude personal workspace for Protected policy/governance work. Gemini for operational Public-Internal tasks. No Z.ai (not doing OSS contributions). No local models (doesn't do technical implementation directly).
>
> [Additional roles omitted for template brevity - RadioAstronomy.io has 6 total roles covering Data Scientist, Grant Writer, Student Support]

---

## 6. Responsibilities

[GUIDANCE: Define what each stakeholder is responsible for. Creates accountability and prevents "I thought someone else was handling that" failures. Common stakeholders: executives, managers, individual contributors, security team, compliance team, AI system owners.]

### 6.X [Stakeholder Group]

[GUIDANCE: For each stakeholder, list 5-10 specific responsibilities. Make them actionable and verifiable.]

**[Stakeholder group name]** is responsible for:

1. [Responsibility 1]
2. [Responsibility 2]
3. [Responsibility 3]
[Continue as needed]

[Repeat for each stakeholder group]

---

> **RadioAstronomy.io Responsibilities:**
>
> ## 6. Responsibilities
>
> [GUIDANCE: RadioAstronomy.io defines 7 stakeholder groups. For template usage, include 4-6 covering typical organizational structure: executives, system owners, users, security/compliance, oversight body.]
>
> ### 6.1 Executive Leadership (CTO)
>
> **Executive Leadership** is responsible for:
>
> 1. **Strategic AI Direction:** Establish organizational AI strategy, approve AI investments, align AI usage with mission objectives
> 2. **Policy Approval:** Review and approve all AI governance policies, standards, and major policy updates
> 3. **Risk Acceptance:** Accept High and Very High residual AI risks; delegate Medium risk acceptance to ARO
> 4. **Resource Allocation:** Ensure adequate budget, staffing, and technical resources for responsible AI implementation
> 5. **Oversight Accountability:** Chair or sponsor AI Review Board; ensure governance processes followed
> 6. **Vendor Relationships:** Approve enterprise AI service contracts; negotiate data protection terms
> 7. **Incident Escalation:** Receive escalation of major AI incidents; approve remediation strategies
> 8. **Cultural Leadership:** Model responsible AI use; promote transparency and ethical integration
>
> ### 6.2 AI Risk Officer (ARO)
>
> **AI Risk Officer** is responsible for:
>
> 1. **Policy Maintenance:** Maintain AI Acceptable Use Policy; update based on operational experience, framework changes, new risks
> 2. **Risk Assessment:** Conduct and review AI system risk assessments; verify compliance with assessment standard
> 3. **Medium Risk Acceptance:** Approve Medium residual risk; escalate High/Very High to CTO/Review Board
> 4. **Tool Validation:** Validate new AI tools against data classification requirements before organizational adoption
> 5. **Training Coordination:** Develop and deliver AI governance training; track competency requirements
> 6. **Incident Response:** Lead AI incident investigations; document lessons learned; update policies/controls
> 7. **Compliance Monitoring:** Monitor adherence to AUP; conduct quarterly audits; report violations
> 8. **Framework Alignment:** Maintain alignment with NIST AI RMF, CIS Controls, Colorado SB24-205, ISO 31000
> 9. **Stakeholder Communication:** Report governance metrics to Review Board; communicate policy changes to organization
>
> ### 6.3 System Owners
>
> **System Owners** (individuals deploying or managing AI systems) are responsible for:
>
> 1. **Risk Assessment:** Conduct risk assessment before deploying new AI systems or making material modifications
> 2. **Data Classification:** Ensure AI system handles only data tiers appropriate for the AI tools used
> 3. **Control Implementation:** Implement controls identified in risk assessment; provide evidence of implementation
> 4. **Access Management:** Request appropriate AI tool access for their role; not share credentials
> 5. **Output Validation:** Review AI outputs for accuracy, bias, hallucination before using in work products
> 6. **Incident Reporting:** Report AI security incidents, policy violations, near-misses to ARO within 24 hours
> 7. **Documentation:** Maintain system documentation; update when system changes; provide to ARO for audits
> 8. **Reassessment:** Conduct periodic reassessment per schedule (quarterly high-risk, annual standard)
> 9. **Transparency:** Disclose AI usage in published research, student-facing services, external communications per Transparency Standard
>
> ### 6.4 All Personnel and AI Agents
>
> **All RadioAstronomy.io personnel and AI agents** are responsible for:
>
> 1. **Policy Compliance:** Follow AI Acceptable Use Policy requirements; adhere to data classification boundaries
> 2. **Tool Selection:** Use only approved AI tools for authorized data classifications and use cases
> 3. **Secrets Protection:** Never input Secrets into any AI model under any circumstances
> 4. **Protected Data Handling:** Use only local models or CTO-isolated Claude for Protected data
> 5. **Output Review:** Validate AI-generated content before use; verify accuracy and appropriateness
> 6. **Incident Reporting:** Report policy violations, security concerns, or inappropriate AI behavior immediately
> 7. **Training Completion:** Complete mandatory AI governance training annually; maintain competency
> 8. **Ethical Use:** Use AI tools ethically and responsibly; do not use for harassment, bias, harm, or policy circumvention
>
> ### 6.5 Security Lead
>
> **Security Lead** is responsible for:
>
> 1. **Technical Controls:** Implement and maintain technical controls for AI risk mitigation (DLP, access controls, monitoring)
> 2. **Architecture Security:** Ensure AI integrations follow secure design principles; review architecture changes
> 3. **Monitoring:** Operate security monitoring for AI systems; detect and alert on anomalous behavior
> 4. **Credential Management:** Enforce secrets management practices; prevent credential exposure to AI
> 5. **Control Verification:** Test control effectiveness; provide evidence for risk assessments and audits
> 6. **Vulnerability Management:** Track and remediate AI-related vulnerabilities; apply security updates
> 7. **Incident Support:** Support ARO in AI incident response; provide technical forensics and remediation
>
> ### 6.6 AI Review Board
>
> **AI Review Board** (quarterly governance oversight body) is responsible for:
>
> 1. **Governance Oversight:** Review AI governance program effectiveness quarterly
> 2. **Very High Risk Approval:** Approve Very High residual risk systems; verify comprehensive controls
> 3. **Policy Review:** Review and approve policy updates proposed by ARO
> 4. **Metrics Review:** Analyze AI usage metrics, incident trends, risk scores; identify patterns
> 5. **Strategic Guidance:** Provide guidance on emerging AI risks, new use cases, framework updates
> 6. **Audit Review:** Review internal audit findings; approve remediation plans
> 7. **Lessons Learned:** Review incident post-mortems; approve policy/control improvements
>
> ### 6.7 Data Protection Officer (if applicable)
>
> [GUIDANCE: Some organizations have DPO role for GDPR compliance. Include if relevant to your org.]
>
> **Data Protection Officer** is responsible for:
>
> 1. **Privacy Compliance:** Ensure AI usage complies with GDPR, CCPA, and applicable privacy regulations
> 2. **PII Handling:** Monitor and restrict PII processing in AI systems; enforce Secrets classification for PII
> 3. **Data Subject Rights:** Ensure AI systems support data subject access, deletion, and portability rights
> 4. **Privacy Impact Assessments:** Conduct or review DPIAs for AI systems processing personal data
> 5. **Vendor Privacy:** Review AI vendor data processing agreements; ensure GDPR-compliant terms

---

## 7. Risk Management and Impact

[GUIDANCE: Connect policy to risk management program. Reference risk scenarios, treatment decisions, and oversight structures. Shows this policy isn't just rules—it's part of systematic risk management.]

### 7.X Risk Scenarios

[GUIDANCE: List key AI risks your organization faces. Common pattern: 5-12 risk scenarios covering: data exposure, credential leakage, prompt injection, infrastructure drift, hallucination/inaccuracy, supply chain, compliance, cost, logging/oversight, operational dependency.]

**Risk scenarios inform policy requirements and control implementation:**

- **[Risk ID]: [Risk Name]** - [Brief description]
  - **Key Controls:** [Controls mitigating this risk from policy]
  - **Residual Risk:** [Accepted risk level]

[Continue for each major risk scenario]

### 7.X Risk Governance Structure

[GUIDANCE: Describe your governance bodies and risk acceptance authority.]

**Decision Authority:**

- [Low risk]: [Who approves]
- [Medium risk]: [Who approves]
- [High risk]: [Who approves]
- [Critical risk]: [Who approves]

**Governance Bodies:**

- **[Body Name]:** [Purpose, frequency, membership, authority]

---

> **RadioAstronomy.io Risk Management:**
>
> ## 7. Risk Management and Impact
>
> ### 7.1 Risk Scenario Summary
>
> [GUIDANCE: RadioAstronomy.io has 12+ risk scenarios. For template usage, include 5-10 most relevant to your operations. Each should reference specific policy controls.]
>
> **Risk scenarios inform policy requirements and control implementation:**
>
> - **R01: Data Egress Exposure** - Protected data submitted to cloud AI services (training, third-party access, regulatory violation)
>   - **Key Controls:** Section 4 (data classification boundaries), Section 5 (tool selection by tier), Section 9.3 (DLP monitoring)
>   - **Residual Risk:** Medium (4/10) with enterprise contracts + DLP + training
>
> - **R02: Secrets Credentials Leakage** - API keys, passwords inadvertently included in prompts or training data
>   - **Key Controls:** Section 4.1 (Secrets never AI), Section 6 (all personnel responsibilities), Section 9.1 (credential scanning)
>   - **Residual Risk:** Low (2/10) with secrets management + scanning + training
>
> - **R03: Prompt Injection Tool Abuse** - Malicious input manipulates AI to perform unintended actions
>   - **Key Controls:** Section 8.2 (input validation training), Section 9.3 (monitoring), Section 6.3 (output review)
>   - **Residual Risk:** Medium (4/10) with input validation + monitoring + human review
>
> - **R07: Hallucination Inaccurate Output** - AI generates plausible but incorrect information affecting research
>   - **Key Controls:** Section 6.3 (output validation), Section 8.4 (research integrity), Section 12 (oversight)
>   - **Residual Risk:** Medium (5/10) with human review + domain expertise + verification workflow
>
> - **R09: Cost Tool Sprawl** - Uncontrolled AI subscriptions create budget waste and shadow IT
>   - **Key Controls:** Section 5 (approved tool catalog), Section 9.2 (expense monitoring), Section 10 (procurement)
>   - **Residual Risk:** Medium (5/10) with centralized procurement + quarterly audits
>
> - **R10: Logging Blind Spots** - Insufficient audit logging prevents incident detection and compliance verification
>   - **Key Controls:** Section 9.3 (audit logging), Section 6.4 (monitoring responsibilities), Section 9.4 (accountability)
>   - **Residual Risk:** Medium (4/10) with comprehensive logging + quarterly reviews
>
> - **R12: Insufficient Oversight Governance** - Inadequate human review, lack of accountability, unclear approval authority
>   - **Key Controls:** Section 6 (clear responsibilities), Section 7.5 (Review Board), Section 12 (success factors)
>   - **Residual Risk:** Low (3/10) with governance structure + quarterly oversight + clear approval matrix
>
> [Full risk scenario details available in separate risk scenario documents R01-R12+]
>
> ### 7.2 Defense-in-Depth Architecture
>
> RadioAstronomy.io uses layered controls ensuring single control failure doesn't create unacceptable risk:
>
> **Layer 1: Preventive (Stop Before It Happens)**
> - Data classification system (Section 4)
> - Architectural isolation (Protected → local only)
> - Tool access controls (Section 5 role-based stack)
> - Enterprise contracts with training opt-out
> - Secrets management (never AI)
>
> **Layer 2: Detective (Catch During or After)**
> - DLP monitoring (Section 9.3)
> - Expense monitoring for shadow AI (Section 9.2)
> - Audit logging of AI usage (Section 9.3)
> - Quarterly license audits (Section 9.3)
> - Incident reporting system (Section 9.2)
>
> **Layer 3: Corrective (Fix When Something Goes Wrong)**
> - Incident response procedures (Section 9.2)
> - Lessons learned documentation (Section 11.2)
> - Policy updates from operational experience (Section 11.1)
> - Shadow AI amnesty and consolidation (R09 treatment)
> - Credential rotation after exposure (R02 treatment)
>
> **Layer 4: Governance (Ensure System Works)**
> - AI Review Board oversight (Section 7.5)
> - Quarterly metrics review (Section 9.4)
> - Annual training (Section 8.1)
> - Risk reassessment schedule (Section 7.3)
> - Executive risk acceptance (Section 7.3)
>
> ### 7.5 AI Review Board Governance
>
> **Purpose:** Quarterly oversight body ensuring AI governance program effectiveness and providing strategic guidance
>
> **Membership:**
> - CTO (Chair)
> - AI Risk Officer
> - Security Lead
> - Research Lead
> - [Additional stakeholders as appropriate]
>
> **Frequency:** Quarterly minimum; emergency meetings as needed for High/Very High risk decisions
>
> **Responsibilities:**
> - Review and approve Very High residual risk acceptances
> - Review governance metrics (AI usage, incidents, training completion, shadow AI discoveries)
> - Approve policy updates proposed by ARO
> - Provide strategic guidance on emerging AI risks and new use cases
> - Review incident post-mortems and approve remediation
> - Monitor framework changes (NIST AI RMF, CIS, Colorado SB24-205) and approve alignment updates
>
> **Decision Authority:**
> - Very High risk systems (9-10/10 residual) require Board approval
> - Policy changes affecting risk tolerance require Board approval
> - Material framework changes require Board approval
> - New AI service categories (not currently in approved stack) require Board approval
>
> **Meeting Outputs:**
> - Meeting minutes documenting decisions and rationale
> - Risk acceptance memos for Very High risk systems
> - Policy update approvals
> - Action items with owners and due dates

---

## 8. Training and Competency

[GUIDANCE: Define training requirements. Who needs training? What topics? How often? How do you verify competency?]

### 8.X Training Requirements

[GUIDANCE: Define mandatory training for different roles/populations.]

**All Personnel:**
- [Training topic 1 - e.g., Data classification and tool selection]
- [Training topic 2 - e.g., Security awareness for AI]
- [Training frequency - e.g., Annual mandatory, or upon hire + annual refresh]

**[Specific Role]:**
- [Additional training for this role]
- [Frequency and verification method]

### 8.X Competency Verification

[GUIDANCE: How do you ensure training was effective?]

- [Method 1 - e.g., Post-training quiz, minimum 80% score]
- [Method 2 - e.g., Practical assessment]
- [Record keeping - e.g., Training completion tracked in HR system]

---

> **RadioAstronomy.io Training:**
>
> ## 8. Training and Competency
>
> ### 8.1 Competency Requirements
>
> **All Personnel (Mandatory Annual Training):**
>
> - **Data Classification System:** Understand four tiers (Secrets/Protected/Public-Internal/Public-External) and decision framework for classifying data
> - **Tool Selection:** Map role to approved AI tools; understand data classification boundaries for each tool
> - **Secrets Protection:** Recognize credentials; understand "never AI" principle; use secrets management systems
> - **Output Validation:** Verify AI-generated content accuracy before use; understand hallucination risks
> - **Incident Reporting:** Recognize policy violations and security concerns; report within 24 hours
> - **Ethical Use:** Responsible AI principles; transparency requirements; research integrity standards
>
> **System Owners (Additional Training):**
>
> - **Risk Assessment Process:** Complete AI Risk Assessment Standard training; understand 6-step CIS-RAM methodology
> - **Control Implementation:** Implement technical controls from risk assessments; verify control effectiveness
> - **Documentation:** Maintain system documentation; conduct reassessments per schedule
>
> **ARO and Security Lead (Specialized Training):**
>
> - **Framework Deep Dive:** NIST AI RMF, CIS-RAM, CIS Controls v8.1, Colorado SB24-205, ISO 31000
> - **Risk Scenario Analysis:** Understand R01-R12+ risk scenarios deeply; apply to new situations
> - **Control Design:** Design and verify controls for novel risks not covered by standard scenarios
> - **Incident Response:** AI-specific incident handling; forensics; lessons learned documentation
>
> **Frequency:**
> - All Personnel: Annual mandatory training + immediate upon policy changes
> - System Owners: Upon taking System Owner role + before conducting first risk assessment + annual refresh
> - ARO/Security Lead: Upon role assignment + framework updates + continuous learning
>
> ### 8.2 Training Delivery Methods
>
> - **Self-Paced Modules:** Data classification, tool selection (Markdown documentation in vault)
> - **Hands-On Workshops:** Risk assessment methodology, control implementation (led by ARO)
> - **Scenario-Based Exercises:** Incident response, policy application (quarterly Review Board reviews real scenarios)
> - **Documentation:** All training materials in vault under `/governance/training/`
>
> ### 8.3 Competency Verification
>
> - **Knowledge Check:** Post-training quiz for all personnel (minimum 80% score); unlimited retakes allowed
> - **Practical Assessment:** System Owners complete risk assessment under ARO supervision before independent assessment authority granted
> - **Continuous Evaluation:** Incident reviews assess whether training effective (incidents reveal training gaps → update curriculum)
> - **Record Keeping:** Training completion tracked in HR system; ARO monitors compliance; quarterly report to Review Board

---

## 9. Enforcement

[GUIDANCE: How is policy enforced? What happens when violations occur? Be specific about consequences while maintaining proportionality.]

### 9.X Monitoring and Audit

[GUIDANCE: Describe monitoring and audit procedures.]

- [Monitoring mechanism 1]
- [Monitoring mechanism 2]
- [Audit frequency and scope]

### 9.X Incident Response

[GUIDANCE: What constitutes violation? How are incidents handled?]

**Violations include:**
- [Violation type 1]
- [Violation type 2]

**Response procedures:**
- [Step 1]
- [Step 2]
- [Step 3]

### 9.X Consequences

[GUIDANCE: Define enforcement actions. Balance accountability with learning culture.]

**Disciplinary actions may include:**
- [Minor violations]: [Consequence]
- [Moderate violations]: [Consequence]
- [Major violations]: [Consequence]

---

> **RadioAstronomy.io Enforcement:**
>
> ## 9. Enforcement
>
> ### 9.1 Monitoring and Audit
>
> **Continuous Monitoring:**
> - **Git Secret Scanning:** Pre-commit hooks scan for credentials in code; blocks commits containing Secrets
> - **DLP Monitoring:** Google Workspace DLP monitors email, Drive, Docs for Protected data patterns; alerts to Security Lead
> - **API Consumption Tracking:** Monthly review of AI service usage (token consumption, API calls, costs by user/project)
> - **Access Logging:** All AI system access logged; quarterly audit of access patterns for anomalies
> - **Network Monitoring:** Egress monitoring detects unauthorized AI service usage outside approved vendor list
>
> **Periodic Audits:**
> - **Quarterly License Audit:** Reconcile approved AI tool registry against actual subscriptions (finance records, vendor portals, network traffic)
> - **Annual Compliance Audit:** ARO conducts comprehensive policy compliance audit; presents findings to Review Board
> - **Shadow AI Survey:** Annual "AI tools you use" survey; identifies unapproved tools; triggers amnesty review
>
> ### 9.2 Incident Response
>
> **Policy Violations Include:**
> - Using unapproved AI tools (shadow AI)
> - Processing Secrets through any AI model
> - Processing Protected data via unauthorized tools (Gemini, Z.ai)
> - Using Z.ai for internal repositories or non-astronomy/data-science work
> - Failing to validate AI outputs before use in published research or student-facing services
> - Circumventing data classification boundaries
> - Failing to report security incidents within 24 hours
> - Sharing AI service credentials
> - Using AI for harassment, bias, or policy circumvention
>
> **Incident Response Procedures:**
>
> 1. **Detection:** Violation detected via monitoring, self-report, peer report, or audit
> 2. **Containment:** Immediate action to stop ongoing violation (revoke access, block service, isolate system)
> 3. **Assessment:** ARO evaluates severity (Low/Medium/High) and impact (actual harm vs potential harm)
> 4. **Investigation:** ARO determines root cause (intentional policy circumvention vs lack of training vs unclear policy)
> 5. **Remediation:** Implement technical fixes (update controls, close gaps, improve monitoring)
> 6. **Communication:** Notify affected stakeholders (students if data exposed, collaborators if research impacted, Review Board if High severity)
> 7. **Lessons Learned:** Document incident and root cause; update policy/training if indicated; present to Review Board
> 8. **Follow-Up:** Verify remediation effective; monitor for recurrence
>
> **Severity Classification:**
> - **Low:** Unintentional minor violation with no data exposure or harm (using wrong tool for Public-Internal data, no sensitive data involved)
> - **Medium:** Significant violation or potential exposure requiring investigation (Protected data sent to Gemini, shadow AI tool discovered)
> - **High:** Secrets exposed, actual data breach, intentional policy circumvention, research integrity compromise, student harm
>
> ### 9.3 Consequences
>
> RadioAstronomy.io takes learning-focused approach to enforcement. Goal is continuous improvement, not punishment. However, consequences escalate with severity and recurrence.
>
> **First-Time Unintentional Violations (Low Severity):**
> - **Action:** Coaching conversation with ARO; remedial training; documentation in incident log
> - **No disciplinary action** if violation resulted from unclear policy, inadequate training, or honest mistake
> - Policy/training updated if violation reveals gap
>
> **Repeat Violations or Medium Severity:**
> - **Action:** Formal written warning; mandatory retraining; temporary access restriction; manager notification
> - **Tool access restrictions:** May lose access to higher-tier tools (e.g., Claude access revoked, limited to Gemini only) for 30-90 days pending demonstrated competency
> - **Escalation:** CTO notified; Review Board informed at next quarterly meeting
>
> **High Severity or Intentional Policy Circumvention:**
> - **Action:** Immediate access suspension; formal investigation; disciplinary action up to termination depending on intent and harm
> - **Data breach consequences:** Follow data breach notification requirements (affected individuals, collaborators, funders, potentially regulatory authorities if PII involved)
> - **Research integrity violations:** Retraction or correction of affected publications; notification to journals/collaborators
> - **Legal consequences:** Depending on violation (credential theft, intentional data exfiltration, sabotage) may involve law enforcement
>
> ### 9.4 Accountability Structures
>
> - **Individual Accountability:** All personnel and AI agents subject to policy; violations tracked per individual; patterns identified
> - **System Owner Accountability:** System Owners accountable for their systems' compliance; risk acceptance decisions create explicit accountability
> - **ARO Accountability:** ARO accountable for governance program effectiveness; reports quarterly metrics to Review Board
> - **Executive Accountability:** CTO accountable for overall program; accepts High/Very High risks; ensures resource adequacy
> - **Review Board Accountability:** Review Board accountable for oversight effectiveness; strategic guidance quality; policy approval decisions
>
> **Quarterly Governance Metrics (Accountability Evidence):**
> - AI tool usage by role and data classification
> - Policy violations count and severity distribution
> - Incident response time (detection to resolution)
> - Training completion rates by role
> - Shadow AI discoveries and resolution
> - Risk assessment coverage (% of systems assessed per schedule)
> - Control implementation effectiveness (residual risk trends)
> - Cost management (actual vs budgeted AI spend)

---

## 10. Success Factors

[GUIDANCE: What makes this policy work in practice? Connect policy to operational culture and outcomes.]

### 10.X Critical Success Factors

[GUIDANCE: List 5-8 factors essential for policy effectiveness.]

1. [Factor 1 - e.g., Executive support and visible leadership]
2. [Factor 2 - e.g., Clear, practical guidance that people can actually follow]
3. [Factor 3 - e.g., Training that builds genuine competency]
[Continue as needed]

---

> **RadioAstronomy.io Success Factors:**
>
> ## 10. Success Factors
>
> ### 10.1 Critical Success Factors
>
> The following factors are essential for this policy's effectiveness:
>
> 1. **Executive Support:** CTO actively uses and models policy compliance; provides resources for implementation; visibly holds people accountable
>
> 2. **Practical Guidance:** Policy provides clear "if-then" decision framework; people can determine correct action without needing AI governance expertise
>
> 3. **Appropriate Tooling:** Approved AI tools actually meet user needs; users aren't driven to shadow AI by inadequate approved options
>
> 4. **Learning Culture:** Incidents treated as learning opportunities; policy updated based on operational experience; non-punitive reporting encouraged
>
> 5. **Technical Enforcement:** Automated controls (DLP, secret scanning, access controls) prevent violations proactively rather than relying solely on training
>
> 6. **Clear Accountability:** Every requirement has named owner; approval authorities explicit; no ambiguity about who decides what
>
> 7. **Integrated Operations:** Policy integrated into existing workflows (git, Obsidian, Google Workspace) rather than requiring separate compliance system
>
> 8. **Measured Effectiveness:** Quarterly metrics show policy working; data-driven improvements; Review Board uses evidence not assumptions
>
> 9. **Stakeholder Trust:** Research community, students, collaborators, funders trust our AI governance; transparency builds confidence
>
> 10. **Continuous Improvement:** Policy evolves through operational experience; not static document; version control shows maturity

---

## 11. Policy Maintenance

[GUIDANCE: How does policy stay current? Define review cycle, update triggers, version control.]

### 11.X Review Cycle

[GUIDANCE: When is policy systematically reviewed?]

- **Regular Review:** [Frequency - e.g., Annual, Semi-annual]
- **Responsible Party:** [Who conducts review]
- **Approval Authority:** [Who approves updates]

### 11.X Update Triggers

[GUIDANCE: What events require immediate policy review outside regular cycle?]

- [Trigger 1 - e.g., Major AI incident or near-miss]
- [Trigger 2 - e.g., New regulatory requirements]
- [Trigger 3 - e.g., Significant framework updates (NIST, ISO, etc.)]
[Continue as needed]

### 11.X Version Control

[GUIDANCE: How are changes tracked and communicated?]

- [Version control system - e.g., Git, document management system]
- [Change notification process]
- [Training requirements when policy changes]

---

> **RadioAstronomy.io Policy Maintenance:**
>
> ## 11. Policy Maintenance
>
> ### 11.1 Review Cycle
>
> - **Regular Review:** Annual comprehensive review led by ARO; presented to Review Board for approval
> - **Quarterly Check:** ARO reviews policy effectiveness metrics quarterly; minor updates as needed
> - **Responsible Party:** AI Risk Officer conducts review; Security Lead provides technical input; CTO approves
> - **Review Board Approval:** Material changes require Review Board approval; minor clarifications can be CTO-approved
>
> ### 11.2 Update Triggers
>
> Policy review required immediately upon:
>
> - **Major AI Incident:** High-severity incident or near-miss revealing policy gap; post-incident review may identify needed policy updates
> - **New Regulatory Requirements:** Colorado SB24-205 amendments, GDPR guidance updates, new AI regulations
> - **Framework Updates:** NIST AI RMF 2.0 release, CIS Controls v9, material ISO 31000 changes
> - **New AI Services:** Organization adopts new AI service categories not covered by current approved stack
> - **Risk Landscape Changes:** New AI risk scenarios identified through operations or industry incidents
> - **Organizational Changes:** Restructuring, new roles, changed mission affecting AI usage patterns
> - **Vendor Changes:** AI service vendor acquired, terms changed, data handling practices modified
> - **Shadow AI Pattern:** Discovery of systematic shadow AI usage indicating approved tools inadequate
> - **Stakeholder Feedback:** Collaborators, funders, students, research community express concerns requiring policy response
>
> ### 11.3 Version Control and Communication
>
> - **Version Control System:** Git repository with full change history; all policy versions preserved
> - **Semantic Versioning:** Major.Minor.Patch (1.0.0 → 2.0.0 for major restructuring, 1.0.0 → 1.1.0 for new sections, 1.0.0 → 1.0.1 for clarifications)
> - **Change Documentation:** Each version includes change summary in Revision History table (Section 12)
> - **Change Notification:**
>   - Minor updates (patches): Email notification to all personnel; 7-day implementation period
>   - Major updates: All-hands presentation by ARO; mandatory retraining; 30-day implementation period with grace period for adaptation
>   - Emergency updates (security): Immediate notification; effective immediately; follow-up training within 48 hours
> - **Accessibility:** Current policy version in vault (`/governance/policies/ai-acceptable-use-policy.md`); synced to all agents via substrate; always accessible
> - **Training Updates:** When policy changes, training materials updated within 14 days; personnel notified of new training availability

---

## 12. Revision History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| [x.x] | [YYYY-MM-DD] | [Description] | [Name/Role] |

[GUIDANCE: Track policy evolution. Shows governance maturity and provides audit trail.]

---

> **RadioAstronomy.io Revision History:**
>
> ## 12. Revision History
>
> | Version | Date | Changes | Author |
> |---------|------|---------|--------|
> | 1.0 | 2024-03-15 | Initial policy establishment: data classification, role-based stack, core governance | CTO + ARO |
> | 2.0 | 2024-09-20 | Major update: Added Z.ai for Public-External, refined Protected data handling, expanded risk scenarios R06-R09 | CTO + ARO + Review Board |
> | 2.1 | 2024-11-10 | Clarified CTO-isolated Claude workspace for Protected data; updated Gemini version to Pro 2.5 | ARO |
> | 3.0 | 2025-01-27 | Comprehensive restructuring: Full framework integration (NIST AI RMF + CIS + ISO 31000 + Colorado SB24-205), expanded risk scenarios to R01-R12, defense-in-depth architecture, AI Review Board formalization | CTO + ARO + Review Board |
>
> ---
>
> **Why version control matters:** Policy evolution shows governance maturity. v3.0 represented major maturity leap - from basic data classification to comprehensive multi-framework compliance. Audit trail demonstrates continuous improvement. Stakeholders (collaborators, funders, students) can see our governance strengthening over time, building trust.

---

## Appendix A: Framework Alignment

[GUIDANCE: Optional but valuable for multi-framework compliance. Shows how policy requirements satisfy multiple frameworks simultaneously. Demonstrates comprehensive governance and reduces duplicate documentation burden.]

[Table or narrative mapping policy sections to framework requirements across multiple standards]

---

> **RadioAstronomy.io Framework Alignment:**
>
> ## Appendix A: Multi-Framework Compliance Mapping
>
> RadioAstronomy.io's AI Acceptable Use Policy satisfies multiple framework requirements through unified policy structure:
>
> | Policy Section | NIST AI RMF 1.0 | CIS Controls v8.1 IG1 | ISO 31000:2018 | Colorado SB24-205 |
> |----------------|-----------------|----------------------|----------------|-------------------|
> | **Section 1: Introduction** | GOVERN 1.1 (Purpose defined), GOVERN 1.2 (Roles established) | CIS 1.1 (Asset Management foundation) | Clause 4 (Organizational context) | §6-1-1704(1)(a) (Risk management program) |
> | **Section 4: Data Classification** | MAP 1.1 (AI system context), GOVERN 1.4 (Risk categorization) | CIS 3.2 (Data inventory), CIS 3.3 (Data classification), CIS 3.11 (Encryption), CIS 3.12 (Segmentation) | Clause 6.4 (Risk treatment via architectural controls) | §6-1-1704(1)(a) (Risk-based approach), §6-1-1703(2)(b) (Transparency) |
> | **Section 5: Role-Based AI Stack** | GOVERN 1.5 (Accountability), MAP 1.2 (Context of use) | CIS 5.1 (Asset Management), CIS 6.8 (Role-based access) | Clause 6.4.2 (Risk treatment options) | §6-1-1704(2)(a) (Training data practices defined), §6-1-1703(2)(b) (Transparency) |
> | **Section 6: Responsibilities** | GOVERN 1.5 (Accountability structures), GOVERN 2.1 (Diverse expertise) | CIS 6.1 (Access control responsibilities), CIS 14.1 (Security awareness roles) | Clause 5.4 (Leadership and commitment) | §6-1-1704(3) (Training responsibilities), §6-1-1704(1)(a) (Risk management roles) |
> | **Section 7: Risk Management** | MAP 1.6 (Risk context), MEASURE 2.3 (Risk assessment), MANAGE 1.1 (Risk responses) | CIS 18.1-18.4 (Risk assessment program) | Clause 6 (Risk assessment and treatment process) | §6-1-1703(2)(a) (Impact assessment), §6-1-1704(1)(a) (Risk management program) |
> | **Section 8: Training** | GOVERN 1.6 (Workforce diversity), GOVERN 3.2 (Training for identified risks) | CIS 14.1 (Security awareness program) | N/A | §6-1-1704(3) (Training requirements for personnel) |
> | **Section 9: Enforcement** | GOVERN 3.2 (Risk tracking), MANAGE 2.3 (Continuous monitoring) | CIS 8.2 (Audit logs collected), CIS 8.5 (Detailed logs), CIS 8.11 (Log reviews), CIS 17.1 (Incident response) | Clause 6.6 (Monitoring and review), Clause 7.4 (Recording and reporting) | §6-1-1704(1)(a) (Compliance monitoring) |
> | **Risk Scenarios (R01-R12+)** | Operationalizes seven trustworthiness characteristics | Maps to specific CIS Controls per scenario | Clause 6.4.2 (Risk treatment catalog) | §6-1-1703(2)(a) (Comprehensive impact considerations) |
>
> ### Single Policy, Multiple Framework Satisfaction
>
> **Example: Section 4.2 (Protected Data Classification)**
>
> One policy section satisfies four frameworks:
>
> - **NIST AI RMF:** MAP 1.1 (system context documented), GOVERN 1.4 (risks categorized by data sensitivity), GOVERN 1.6 (third-party risk addressed via tool restrictions)
> - **CIS Controls:** CIS 3.2 (Protected data inventoried), CIS 3.12 (processing segmented by classification), CIS 9.2 (DLP enforces boundaries)
> - **ISO 31000:** Clause 6.4 (risk treatment via architectural isolation - local models or CTO workspace), Clause 6.4.2 (treatment option selection rationalized)
> - **Colorado SB24-205:** §6-1-1704(1)(a) (risk management addresses data sensitivity), §6-1-1703(2)(b) (transparency regarding training data practices - training opt-out or local processing)
>
> This integration means:
> - No duplicate policies for different frameworks
> - Single compliance audit covers multiple requirements
> - Control implementation satisfies multiple mandates simultaneously
> - Governance decisions traceable to multiple framework sources
> - Organizational burden minimized while coverage maximized
>
> ### Framework Maintenance
>
> When frameworks update (NIST AI RMF 2.0, CIS Controls v9, Colorado SB24-205 amendments), ARO reviews alignment table and updates policy if material changes required. Quarterly Review Board meetings include framework alignment verification.

---

**END OF POLICY**

---

## Template Adoption Guide

[GUIDANCE FOR ORGANIZATIONS USING THIS TEMPLATE:]

**Dual-Layer Structure:**

1. **Template Guidance (plain text):** Questions to answer, decisions to make, organizational considerations, customization points
2. **RadioAstronomy.io Implementation (block quotes):** Working example from 6-person citizen science organization showing operational reality with reasoning

**How to Use This Template:**

1. **Read RadioAstronomy.io blocks:** Understand concrete implementation before adapting. Provides reference point for decision-making.

2. **Assess fit:** Is your situation similar? What's different? Consider:
   - Scale (6 vs 60 vs 600 people)
   - Risk profile (downstream impact, regulatory requirements, stakeholder expectations)
   - Data sensitivity range (what's your most/least sensitive data?)
   - AI integration level (occasional tools vs AI-native operations)
   - Compliance drivers (mandatory regulation vs voluntary best practice)

3. **Make your decisions:**
   - **Data Classification:** Choose 3-5 tiers appropriate for your data sensitivity range
   - **Role-Based Stack:** Define 3-7 roles covering your organizational functions
   - **Frameworks:** Select compliance frameworks matching your requirements (NIST/ISO/CIS/regulatory)
   - **Risk Scenarios:** Develop 5-12 scenarios covering your operational risks
   - **Governance Structure:** Define oversight bodies and approval authorities appropriate to your org structure

4. **Document your implementation:** Replace [brackets] with your choices; write your own implementation blocks explaining operational decisions

5. **Keep or remove RadioAstronomy.io examples:** Your choice - some orgs keep as reference, others delete after adapting

**Framework Selection Patterns:**

**RadioAstronomy.io chose:**
- **NIST AI RMF 1.0:** Comprehensive risk framework (non-regulatory, voluntary)
- **CIS Controls v8.1 IG1:** Small org cybersecurity baseline (free, prescriptive)
- **ISO 31000:2018:** Risk management principles (governance structure)
- **Colorado SB24-205:** Legal reference (voluntary compliance for impact transparency)

**Your alternatives:**

- **Enterprise/ISO shop:** ISO 42001 (AI management) + ISO 31000 (risk) + ISO 27001 (security) - better for large governance structures
- **Federal/Defense:** NIST AI RMF + NIST SP 800-30 (risk) + NIST SP 800-53 (controls) - required for government work
- **EU operations:** EU AI Act requirements + ISO 42001 + GDPR alignment - regulatory mandate
- **Financial services:** NIST AI RMF + OCC guidance + existing GRC framework - regulatory examination focus
- **Healthcare:** HIPAA-aligned framework + NIST AI RMF + your existing compliance program - regulatory constraints
- **Startup agility:** Lightweight qualitative approach + essential controls + rapid iteration - speed-focused

**Data Classification Adaptation:**

**RadioAstronomy.io uses 4 tiers:** Secrets (never AI) / Protected (local only) / Public-Internal (enterprise cloud) / Public-External (any cloud including training)

**Your alternatives:**
- **3-tier simple:** Public / Internal / Confidential
- **5-tier government:** Public / Internal / Confidential / Sensitive / Top Secret
- **Healthcare-specific:** Public / Internal / PHI / De-identified PHI
- **Financial-specific:** Public / Internal / Customer NPI / Cardholder Data

**The pattern works regardless:** Always have a "never AI" tier for credentials/sensitive data, intermediate tiers with appropriate controls, and broadest tier for public data.

**Role-Based Stack Adaptation:**

**RadioAstronomy.io has 6 roles** mapped to data access patterns and approved tools.

**Your alternatives:**
- **Flat model:** Everyone same tools; rely on training + data classification (works for <10 people, high trust)
- **Department-based:** Marketing ≠ Engineering ≠ Finance stacks (works when departments have distinct data)
- **Clearance-based:** Tools tied to data clearance levels (works for government/defense)
- **Use-case-based:** Tools approved for workflows not roles (works when roles fluid, workflows stable)

**Policy Maintenance:**

- Annual review minimum
- Update immediately for incidents, regulatory changes, framework updates, new risks
- Version control with clear change documentation
- Training updates when policy changes materially
- Continuous improvement based on operational experience

**Questions While Adapting:**

- "Our data is different" → Adapt classification tiers; keep pattern (never AI tier + intermediate tiers with controls + public tier)
- "Our risk scenarios differ" → Build your own R01-R0X catalog; RadioAstronomy.io's are examples not prescriptions
- "We don't have Review Board" → Adapt governance to your structure; keep principle of executive oversight and risk acceptance authority
- "Our tools are different" → Replace Gemini/Claude/Z.ai with your approved tools; keep role-based mapping pattern
- "We need more/fewer tiers" → Adjust classification system; common range is 3-5 tiers
- "We can't do local models" → Adapt Protected data handling; options include isolated cloud workspaces, enhanced vendor agreements, or elevated classification (Protected becomes Secrets/never AI)

**Success Criteria:**

- Policy clear enough that users can determine correct action without AI governance expertise
- Data classification boundaries enforceable with technical controls
- Role-based stack matches actual work patterns
- Risk scenarios reflect operational reality
- Governance structure provides oversight without bureaucratic burden
- Training builds genuine competency
- Policy sustainable - team actually follows it without excessive burden
- Compliance demonstrable to auditors/stakeholders

**Adapt to your context.** RadioAstronomy.io's implementation shows it works at small scale with high rigor. Your organization's size, risk profile, regulatory requirements, and technical maturity will drive different choices. The framework works at any scale - customize the specifics.
