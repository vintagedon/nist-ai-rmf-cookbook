# Model Card: Local Models (Proxmox Lab Deployment)
# Following model-card.schema.yaml for NIST AI RMF Cookbook documentation

schema_version: 1.0.0

model_details:
  name: "Local LLM Deployment"
  version: "Variable (Ollama-managed, multiple models)"
  owner: "VintageDon (self-hosted)"
  license: "Various open-source licenses depending on specific model"
  description: |
    A collection of smaller open-source language models deployed locally on NVIDIA RTX A4000
    hardware within the Proxmox Astronomy Lab infrastructure. These models provide offline
    capability, complete data control, and lightweight review functionality for the NIST AI
    RMF Cookbook repository, complementing the frontier commercial models (Claude, GPT-4, Gemini).
    
    This deployment demonstrates governance of self-hosted AI infrastructure and serves as a
    practical example of local AI deployment for organizations with data sovereignty requirements.
  
  model_url: "https://github.com/Pxomox-Astronomy-Lab/proxmox-astronomy-lab"
  
  model_architecture:
    family: "Multiple families (typically LLaMA, Mistral, or similar open-source architectures)"
    parameter_count: "Typically 7B-13B parameters (quantized to fit 16GB VRAM)"
    context_window: "4k-32k tokens depending on specific model"
    modalities: "Text input and output"
    quantization: "4-bit or 8-bit quantization to optimize for VRAM constraints"
    
  deployment_stack:
    orchestration: "Kubernetes (part of Proxmox Astronomy Lab cluster)"
    inference_runtime: "Ollama"
    hardware: "NVIDIA RTX A4000 (16GB VRAM)"
    host_system: "Proxmox VE virtualization platform"
    
  training_details:
    training_data_cutoff: "Varies by model - typically 6-18 months behind frontier models"
    training_data_sources: "Depends on specific model selected"
    training_methodology: "Models are pre-trained by original authors; no local fine-tuning"
    local_modifications: "None - using models as-is from Ollama model library"

intended_use:
  primary_uses: |
    In the NIST AI RMF Cookbook repository, local models are used for:
    - Light syntax and structure checking of documentation
    - Offline development when internet connectivity unavailable
    - Quick iterative testing of documentation patterns
    - Privacy-sensitive draft work before public disclosure
    - Experimentation with prompts and templates
    - Learning about local model deployment and governance
  
  primary_users: "Repository maintainer (VintageDon) only - no shared access"
  
  out_of_scope_uses: |
    Local models are NOT used for:
    - Primary content creation (quality insufficient compared to frontier models)
    - Critical framework mappings or compliance interpretations
    - Final documentation generation
    - Complex reasoning tasks requiring high accuracy
    - Multi-model consensus (only frontier models participate)
  
  usage_context: |
    Local models provide a controlled, privacy-preserving environment for initial drafts
    and experimentation. All content generated by local models requires additional review
    and verification before use in repository. These models demonstrate practical local
    AI deployment as part of the repository's infrastructure documentation.

evaluation:
  evaluation_data: |
    Evaluated through practical use for light documentation tasks. No formal benchmarking
    performed; assessment is entirely qualitative and relative to frontier models.
  
  metrics:
    - name: "Offline Availability"
      value: "100% (no internet dependency)"
      description: "Provides complete independence from commercial API availability"
    
    - name: "Syntax Checking Quality"
      value: "Adequate for basic review"
      description: "Can catch obvious markdown syntax errors and formatting issues"
    
    - name: "Content Generation Quality"
      value: "Lower than frontier models"
      description: "Outputs require significant human revision; not suitable for final content"
    
    - name: "Privacy Control"
      value: "Complete"
      description: "All data remains on local infrastructure; zero third-party exposure"
  
  strengths:
    - "Complete data sovereignty - nothing leaves local infrastructure"
    - "No API costs or rate limits"
    - "Offline capability for development without connectivity"
    - "Immediate availability for quick experimentation"
    - "Demonstrates local AI deployment governance"
    - "Useful for initial drafts and brainstorming"
  
  limitations:
    - "Significantly lower quality than frontier models (Claude, GPT-4, Gemini)"
    - "Limited by 16GB VRAM constraint (max ~13B parameter models, quantized)"
    - "Smaller context windows than commercial alternatives"
    - "Slower inference speed than cloud APIs"
    - "No multimodal capabilities"
    - "Requires manual model management and updates"
    - "Cannot participate in multi-model consensus due to quality gap"

ethical_considerations:
  known_biases: |
    Biases vary by specific open-source model deployed. Common concerns include:
    - Open-source models may have less extensive bias mitigation than commercial models
    - Training data biases inherent to the base model
    - Smaller models may have less nuanced understanding of sensitive topics
    
    In the repository context, these biases are mitigated by:
    - Local models only used for lightweight, non-critical tasks
    - All outputs subject to human review before any use
    - Frontier models (Claude, GPT-4, Gemini) used for actual content generation
  
  mitigation_strategy: |
    Repository methodology limits local model bias impact through:
    1. Restricted use cases (syntax checking, experimentation only)
    2. Mandatory human verification of all outputs
    3. Frontier models used for all critical content
    4. Local models explicitly excluded from multi-model consensus
    5. Documentation of limitations in methodology
  
  fairness_considerations: |
    Local model deployment demonstrates accessibility considerations:
    - Shows organizations with limited budgets can deploy local AI
    - Provides privacy-preserving option for sensitive drafting
    - Illustrates governance of self-hosted infrastructure
    - Documents realistic constraints and trade-offs
  
  transparency_approach: |
    Full transparency maintained through:
    - Documentation in local AI infrastructure document
    - Disclosure of quality limitations
    - Clear boundaries on appropriate use cases
    - This model card documenting deployment details
    - Public documentation of infrastructure at GitHub repository

risk_assessment:
  identified_risks:
    - risk_id: "LOCAL-RISK-001"
      category: "Quality / Accuracy"
      description: "Lower quality outputs may introduce errors if used for critical content"
      likelihood: "High if misused"
      impact: "High"
      mitigation: "Strict policy limiting use to non-critical tasks; mandatory verification before any repository use"
    
    - risk_id: "LOCAL-RISK-002"
      category: "Infrastructure Availability"
      description: "Local hardware failure could disrupt availability"
      likelihood: "Low"
      impact: "Low (frontier models provide backup)"
      mitigation: "Frontier models available as primary tools; local models only for supplementary tasks"
    
    - risk_id: "LOCAL-RISK-003"
      category: "Security / Model Integrity"
      description: "Self-hosted models require security maintenance and updates"
      likelihood: "Low with proper management"
      impact: "Medium"
      mitigation: "Models sourced from trusted Ollama library; regular security updates; network isolation"
    
    - risk_id: "LOCAL-RISK-004"
      category: "Resource Contention"
      description: "AI inference may compete with astronomy research workloads"
      likelihood: "Low"
      impact: "Low"
      mitigation: "Kubernetes resource limits; astronomy research takes priority; AI use is intermittent"
    
    - risk_id: "LOCAL-RISK-005"
      category: "Overreliance"
      description: "Convenience might lead to using local models for tasks requiring frontier quality"
      likelihood: "Medium"
      impact: "Medium"
      mitigation: "Clear documentation of limitations; methodology requires frontier models for critical work"

  nist_ai_rmf_alignment:
    govern: "Local deployment governed by same policy as commercial models; demonstrates infrastructure governance"
    map: "Risks identified and documented in this model card"
    measure: "Qualitative assessment through practical use; explicit quality comparison to frontier models"
    manage: "Mitigations through strict use case limitations, human verification, and frontier model backup"

model_usage_guidelines:
  recommended_practices:
    - "Use only for lightweight, non-critical tasks"
    - "Treat all outputs as drafts requiring verification"
    - "Default to frontier models (Claude, GPT-4, Gemini) for any important work"
    - "Experiment freely - failure has low cost in isolated environment"
    - "Document learnings about local model deployment"
  
  prompt_engineering_notes: |
    Local models require more careful prompting than frontier models:
    - Be very explicit and specific in instructions
    - Use shorter, simpler prompts
    - Break complex tasks into smaller steps
    - Set expectations lower for output quality
    - Be prepared to iterate multiple times
  
  when_to_use_local_models:
    - "Quick syntax or format checks"
    - "Offline development without internet access"
    - "Initial brainstorming or outlining"
    - "Experimentation with prompt patterns"
    - "Privacy-sensitive early drafts"
    - "Learning about local AI deployment"
  
  when_not_to_use:
    - "Any critical content generation"
    - "Framework interpretations or compliance guidance"
    - "Final documentation before repository submission"
    - "Tasks requiring high accuracy"
    - "Multi-model consensus processes"

deployment_info:
  deployment_environment: "Proxmox Astronomy Lab (https://github.com/Pxomox-Astronomy-Lab/proxmox-astronomy-lab)"
  serving_infrastructure: |
    Kubernetes cluster running on Proxmox VE:
    - NVIDIA RTX A4000 (16GB VRAM) dedicated to AI workloads
    - Ollama for model management and inference
    - GPU scheduling via Kubernetes device plugins
    - Network-isolated from public internet
  
  hardware_specifications:
    gpu: "NVIDIA RTX A4000"
    vram: "16GB"
    vram_constraint: "Limits to ~13B parameter models with 4-bit quantization"
    compute_capability: "8.6"
    
  deployment_date: "2024-10 (repository project start)"
  
  model_management:
    model_source: "Ollama model library (https://ollama.ai/library)"
    model_updates: "Manual - maintainer pulls new versions as needed"
    model_selection_criteria: "Balance of capability, VRAM requirements, and license compatibility"
    typical_models: |
      Examples of models used (varies over time):
      - Mistral 7B
      - LLaMA 2 7B/13B
      - Gemma 7B
      - Other open-source models under 13B parameters
  
  access_control:
    access_restriction: "Repository maintainer only (VintageDon)"
    authentication: "Local network access only; no public-facing endpoints"
    audit_logging: "Via Kubernetes and system logs"
  
  monitoring_approach: |
    Informal monitoring:
    - Manual observation of inference performance
    - Qualitative assessment of output usefulness
    - Resource usage via Kubernetes metrics
    - No formal quality metrics or automated monitoring
  
  backup_strategy: |
    If local models unavailable:
    - Frontier models (Claude, GPT-4, Gemini) serve as primary tools
    - No critical dependency on local deployment
    - Re-deployment straightforward via Ollama
  
  resource_management: |
    Kubernetes resource limits:
    - GPU allocated to AI pods when requested
    - Astronomy research workloads take priority
    - AI inference runs during low-utilization periods
    - Resource quotas prevent interference with primary research mission

additional_information:
  contact_information: "Repository maintainer: VintageDon (GitHub: @vintagedon)"
  
  related_infrastructure:
    - name: "Proxmox Astronomy Lab"
      description: "Parent infrastructure for AI deployment"
      url: "https://github.com/Pxomox-Astronomy-Lab/proxmox-astronomy-lab"
  
  related_models:
    - name: "Claude Sonnet 4.5"
      relationship: "Primary model - higher quality alternative"
      url: "../claude-sonnet-45-repository-use.yaml"
    
    - name: "GPT-4"
      relationship: "Frontier alternative with superior capabilities"
      url: "../gpt-4-repository-use.yaml"
    
    - name: "Gemini Pro"
      relationship: "Frontier alternative with long-context advantage"
      url: "../gemini-pro-repository-use.yaml"
  
  infrastructure_governance_value: |
    This local deployment serves multiple purposes:
    - Demonstrates practical local AI deployment for documentation
    - Provides concrete example of infrastructure governance
    - Shows organizations with privacy requirements have options
    - Documents realistic constraints and trade-offs
    - Illustrates Kubernetes-based AI deployment patterns
  
  cost_analysis: |
    Hardware investment: NVIDIA RTX A4000 (~$1000 one-time)
    Operating costs: Electricity, hosting (marginal - shared with astronomy research)
    API cost savings: Minimal (frontier models still primary; light usage of local)
    
    Primary value is NOT cost savings but rather:
    - Privacy/data sovereignty
    - Offline capability
    - Learning and demonstration
    - Infrastructure governance example
  
  regulatory_compliance: |
    Deployment aligns with:
    - Repository AI Acceptable Use Policy
    - NIST AI RMF GOVERN function (documented infrastructure governance)
    - Privacy best practices (data never leaves local control)
    - Open-source license compliance (models used per license terms)
  
  model_card_authors:
    - "VintageDon (Repository Maintainer)"
    - "Claude Sonnet 4.5 (Document Structuring)"
  
  model_card_version: "1.0"
  
  changelog:
    - version: "1.0"
      date: "2025-01-XX"
      changes: "Initial model card creation documenting local model deployment in Proxmox Lab"
