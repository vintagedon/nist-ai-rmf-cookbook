schema_version: 1.0.0

model_details:
  name: "Llama 3.1 8B Instruct - Documentation Linter"
  version: "llama3.1:8b-instruct-q4_K_M"
  owner: "AI Core Platform Team (VintageDon)"
  license: "Llama 3.1 Community License"
  description: "Llama 3.1 8B Instruct model quantized to Q4_K_M for documentation structure validation and link checking. Deployed locally via Ollama on NVIDIA RTX A4000 (16GB VRAM). Serves as automated QA copilot for NIST AI RMF Cookbook articles before human review."
  model_url: "https://ollama.com/library/llama3.1:8b-instruct-q4_K_M"
  deployment_context:
    hardware: "NVIDIA RTX A4000 16GB VRAM"
    runtime: "Ollama (local inference)"
    quantization: "Q4_K_M (4-bit, ~4.5GB VRAM footprint)"
    context_window: "128k tokens (effective: ~32k for linting tasks)"

intended_use:
  primary_uses: "Automated structural validation of repository documentation against templates. Checks for mandatory sections, correct ordering, placeholder replacement, style guide conformance, and link validity. Outputs tiered feedback (ERRORS, WARNINGS, SUGGESTIONS) with proposed fixes for broken links."
  primary_users: "Repository maintainer (VintageDon) triggering batch linting runs before manual review. Secondary: future contributors via pre-merge CI hooks."
  out_of_scope_uses: |
    - Content quality assessment (human judgment required)
    - Factual accuracy verification (requires domain expertise)
    - Final editorial decisions (model is advisory only)
    - Real-time interactive editing assistance
    - Multi-document cross-referencing beyond link checking
    - Security or compliance validation (different tooling)

evaluation:
  evaluation_data: "PENDING - Adversarial test suite under development. Will include: (1) Intentionally malformed articles, (2) Perturbation tests (valid variations), (3) Confabulation detection (false positive scenarios)."
  metrics:
    - name: "Section Detection Accuracy"
      value: "PENDING"
      description: "Percentage of mandatory sections correctly identified as present/missing. Target: >= 90%"
      methodology: "Golden test set of 20 articles (10 compliant, 10 with known violations)"
    - name: "Link Fix Accuracy"
      value: "PENDING"
      description: "Percentage of broken links where suggested fix is correct. Target: >= 80%"
      methodology: "Test corpus of 50 intentionally broken links with known correct paths"
    - name: "False Positive Rate"
      value: "PENDING"
      description: "Percentage of flagged issues that are not actual violations. Target: < 10%"
      methodology: "Human review of 100 linter outputs on valid articles"
    - name: "Output Format Conformance"
      value: "PENDING"
      description: "Percentage of responses that are valid JSON matching expected schema"
      methodology: "Automated schema validation on 200 inference runs"

ethical_considerations:
  known_biases: |
    Model may over-index on formatting patterns seen in training data (e.g., Stack Overflow, GitHub READMEs).
    Potential to flag non-Western documentation styles (e.g., different heading conventions) as violations.
    No training data exposure to this repository's specific templates, so may misinterpret novel structures initially.
  mitigation_strategy: |
    - Templates and style guide explicitly embedded in system prompt to override training biases
    - Human-in-loop: All outputs are reviewed by maintainer before enforcement
    - Tiered feedback (errors/warnings/suggestions) allows graceful degradation
    - Manual triggering prevents automated enforcement without oversight
    - Feedback loop: Maintainer corrects false positives, informs prompt refinement

operational_constraints:
  performance:
    inference_speed: "~15-30 tokens/sec (A4000, Q4 quantization)"
    typical_runtime: "30-90 seconds per article (depends on length + repo-tree size)"
    batch_processing: "Manual trigger, processes entire pending queue sequentially"
  reliability:
    failure_modes:
      - "JSON parse errors if model generates malformed output (rate: <5% observed)"
      - "Context window overflow if article + repo-tree exceeds 32k tokens (rare)"
      - "VRAM exhaustion under concurrent load (single-threaded by design)"
    error_handling: "Failed articles logged, skipped, flagged for manual review"
  
deployment_workflow:
  trigger: "Manual command: `make lint-docs` or `python tools/lint_articles.py`"
  inputs:
    - "Repo-tree snapshot (repo-tree.txt or live generation)"
    - "Article content (Markdown file)"
    - "Template context (baked into system prompt)"
  outputs:
    - "JSON feedback file per article (errors/warnings/suggestions)"
    - "Summary report (total articles, pass/warn/fail counts)"
  integration_point: "Pre-human-review gate. Does NOT block merges, only advises."

continuous_improvement:
  prompt_versioning: "System prompt stored in `_repository-governance/prompts/doc-linter-v1.0.txt`"
  evaluation_cadence: "Quarterly review of false positive/negative rates"
  feedback_loop: "Maintainer logs issues in `_repository-governance/linter-feedback.md` for prompt refinement"
  model_replacement_trigger: "If false positive rate exceeds 15% or new Llama version offers significant improvement"

risk_assessment:
  residual_risks:
    - risk_id: "LINTER-RISK-001"
      description: "Model incorrectly flags valid articles, creating friction for contributors"
      likelihood: "Medium"
      impact: "Low (human review prevents enforcement)"
      mitigation: "Tiered feedback + manual trigger limits impact"
    - risk_id: "LINTER-RISK-002"
      description: "Model misses actual violations, allowing broken docs to pass"
      likelihood: "Medium"
      impact: "Low (human review is final gate)"
      mitigation: "Adversarial testing + evaluation metrics track performance"
    - risk_id: "LINTER-RISK-003"
      description: "Model suggests incorrect link fixes, wasting maintainer time"
      likelihood: "Low"
      impact: "Very Low (suggestions are advisory)"
      mitigation: "Link fix validation in evaluation suite"

compliance_notes:
  nist_ai_rmf_alignment:
    govern: "Model usage documented, human oversight required, clear scope boundaries"
    map: "Risks identified (false positives/negatives, incorrect suggestions)"
    measure: "Evaluation plan defined (pending execution)"
    manage: "Mitigation strategies in place (human review, tiered output, feedback loop)"
  data_privacy: "No PII or sensitive data processed. Articles are public repository content."
  model_transparency: "Open-source model (Llama 3.1), quantization method documented, deployment stack specified"