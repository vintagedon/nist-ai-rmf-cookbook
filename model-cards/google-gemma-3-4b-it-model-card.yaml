# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"
# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "Gemma 3 4B-IT"
  vendor: "Google"
  model_family: "Gemma 3"
  version: "4B-IT"
  release_date: "2025"
  model_type: "Open-Weight Multimodal Model (Instruction-Tuned)"
  vendor_model_card_url: "https://huggingface.co/google/gemma-3-4b-it"
  license: "Gemma Terms of Use"
  deprecation_status: "Active"
# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Transformer-based"
    parameter_count: "~4B"
    context_window: "128,000 tokens"
    training_data_cutoff: "Not publicly disclosed"
    architectural_details: |
      A mid-size model in the Gemma 3 family, offering a balance between the on-device focus of the 1B model and the higher performance of the larger variants. [4]
  modalities:
    supported_inputs:
    - "text"
    - "image"
    supported_outputs:
    - "text"
  performance_characteristics:
    speed_tier: "Optimized for deployment on consumer hardware like laptops and desktops."
    cost_tier: "N/A (Open-weight)"
    latency: "Dependent on deployment hardware and configuration."
    throughput: "Suitable for interactive applications on consumer-grade GPUs."
# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    A multimodal model with a large 128K context window and multilingual support, well-suited for a variety of text generation and image understanding tasks on local hardware. [4]
  benchmark_performance: |
    - HellaSwag (10-shot): 77.2
    - TriviaQA (5-shot): 65.8
    - MMLU (5-shot): 59.6
    - HumanEval (0-shot): 36.0
    - Full benchmark table available in the model card. [4]
  special_capabilities:
    tools_support: false
    vision_support: true
    reasoning_support: true
    image_generation: false
    additional_capabilities:
    - "long_context_window"
    - "multilingual_support"
  known_limitations:
    vendor_disclosed: |
      Subject to standard LLM limitations including potential for factual inaccuracies, reflecting biases from training data, and difficulty with highly complex reasoning. [4]
    common_failure_modes: |
      May struggle with tasks requiring deep, specialized knowledge not well-represented in the training data.
    unsuitable_use_cases: |
      High-stakes, safety-critical decisions without human oversight. Any use that violates the Gemma Prohibited Use Policy.
# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Trained on 4 trillion tokens from a dataset including web documents, code, mathematics, and images. [27]
  training_methodology: |
    Trained using JAX and ML Pathways on Google TPU hardware. [4]
  data_privacy_considerations: |
    Training data underwent filtering for CSAM and sensitive data. [4]
# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    Democratizing access to state-of-the-art AI by making it possible to deploy in environments with limited resources such as laptops, desktops, or private cloud infrastructure. [4]
  suitable_domains:
  - "local_ai_assistants"
  - "desktop_applications"
  - "private_cloud_deployments"
  - "research_and_development"
  out_of_scope_use: |
    Use in regulated domains without thorough validation and implementation of safety controls.
# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      Provides superior performance to other, comparably-sized open model alternatives. [4]
    public_evidence: |
      Published benchmarks provide evidence of its capabilities. [4]
    assessment_notes: |
      A capable mid-size model suitable for a wide range of local applications.
  safe:
    safety_measures: |
      Underwent safety evaluations for child safety, content safety, and representational harms. [4]
    known_safety_issues: |
      Can reflect biases from training data and be misused for generating harmful content.
    assessment_notes: |
      Deployers are responsible for implementing safety guardrails.
  secure_and_resilient:
    security_features: |
      Local deployment allows for full control over the security environment.
    known_vulnerabilities: |
      Standard LLM vulnerabilities.
    assessment_notes: |
      Security is the responsibility of the deployer.
  accountable_and_transparent:
    transparency_level: "High"
    auditability: |
      Model weights are open, and the model card provides details on training and evaluation.
    assessment_notes: |
      High transparency enables accountability.
  explainable_and_interpretable:
    explainability_features: |
      Open weights enable research into model internals.
    interpretability_limitations: |
      Full interpretability remains a research challenge.
    assessment_notes: |
      More interpretable than closed-API models.
  privacy_enhanced:
    privacy_features: |
      Local deployment ensures user data does not need to be sent to a third-party API.
    privacy_concerns: |
      Potential for memorization of training data.
    assessment_notes: |
      Strong choice for applications with strict data privacy requirements.
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Data filtering and safety evaluations were performed. [4]
    known_biases: |
      May exhibit biases present in the training data.
    assessment_notes: |
      Requires use-case-specific bias testing.
# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    - Long-context retrieval and summarization tests utilizing the 128K context window.
    - Performance evaluation on consumer-grade GPUs to establish hardware requirements.
    - Multimodal question-answering on domain-specific images.
  key_evaluation_questions: |
    - Can our local infrastructure support the resource requirements of this model?
    - Does the 128K context window provide a significant advantage for our long-document analysis tasks?
  comparison_considerations: |
    Compare against other leading open-weight models in the 3-5B parameter range on long-context performance, multimodal capabilities, and resource efficiency.
# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://huggingface.co/google/gemma-3-4b-it"
    description: "Hugging Face Model Card for Gemma 3 4B-IT"
  - url: "https://goo.gle/Gemma3Report"
    description: "Gemma 3 Technical Report"
