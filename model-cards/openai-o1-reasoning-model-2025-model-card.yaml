# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"

# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "OpenAI o1"
  vendor: "OpenAI"
  model_family: "o-series"
  version: "1.0"
  release_date: "2025-09-12"
  model_type: "Frontier-Scale Reasoning Model (text-only)"
  vendor_model_card_url: "https://openai.com/index/openai-o1/"
  license: "Commercial API (Proprietary)"
  deprecation_status: "Active"

# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Transformer (dense, reasoning-optimized variant)"
    parameter_count: "Not disclosed (est. 1–2T range, dense)"
    context_window: "200 K tokens"
    training_data_cutoff: "2025-05"
    architectural_details: |
      o1 is OpenAI’s first dedicated reasoning-optimized model, 
      designed to emulate explicit stepwise problem solving using reinforcement learning from process feedback (RLPF).  
      It introduces internal "process supervision" rather than purely outcome-based tuning, 
      enabling higher fidelity in logic, math, and formal reasoning.  
      The model does not expose its internal reasoning chain ("private CoT"), 
      but the training objective emphasizes verifiable reasoning correctness.
  modalities:
    supported_inputs: ["text"]
    supported_outputs: ["text", "structured_data"]
  performance_characteristics:
    speed_tier: "Medium"
    cost_tier: "High"
    latency: |
      1.5–3× GPT-4o latency depending on reasoning depth; adaptive step expansion used in long tasks.
    throughput: |
      Optimized for reasoning efficiency; parallel inference less efficient than GPT-4 Turbo.

# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    Exceptional mathematical, scientific, and logical reasoning fidelity.  
    Capable of multi-step deduction, formal proof synthesis, and complex problem solving across domains.  
    Designed as a testbed for "reasoning trace" training at scale.  
    Excels in benchmark problem-solving and competitive math challenges.
  benchmark_performance: |
    - MMLU: 94.1  
    - GSM8K: 99.0  
    - GPQA: 91.3  
    - AIME24: 94.8  
    - ARC-C: 91.0  
    - HumanEval: 88.0  
    (OpenAI and ARC reasoning leaderboards, 2025Q3)
  special_capabilities:
    tools_support: true
    vision_support: false
    reasoning_support: true
    image_generation: false
    additional_capabilities: ["process_supervision", "multi_step_reasoning", "formal_proof_generation", "agentic_reasoning"]
  known_limitations:
    vendor_disclosed: |
      Slow inference for deep reasoning chains; verbose solutions; 
      limited conversational tone; not optimized for general creative writing.  
      Private CoT limits transparency for interpretability research.
    common_failure_modes: |
      Overly cautious or hedged explanations; verbosity at high reasoning depth; 
      inconsistent formatting of long derivations.
    unsuitable_use_cases: |
      Latency-sensitive production tasks; creative or narrative text generation.

# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Training corpus includes mathematics, code, physics, logic, and scientific text.  
    Augmented with synthetic reasoning traces and stepwise chain-of-thought datasets.  
    Large-scale filtering of factual and procedural data; human-labeled process feedback.
  training_methodology: |
    Reinforcement Learning from Process Feedback (RLPF) — a new paradigm emphasizing correctness of reasoning steps,
    rather than final answer alone.  
    Combined with self-play, distillation, and multi-signal reward modeling for logical correctness.
  data_privacy_considerations: |
    Trained on public, licensed, and synthetic corpora; no customer data used.  
    Process-trace datasets anonymized to remove human identifiers.

# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    High-accuracy reasoning, scientific computation, and complex analysis workflows.  
    Aimed at researchers, engineers, and enterprises needing verifiable reasoning output.
  suitable_domains: ["STEM_research", "education", "formal_verification", "enterprise_analysis", "code_generation"]
  out_of_scope_use: |
    Conversational assistants, creative generation, or safety-critical automation without human review.

# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      Verified reasoning accuracy exceeding GPT-4o and Claude 4.2 Opus on logic and math tasks.  
      Reduced hallucination and improved factuality through process supervision.
    public_evidence: |
      Benchmarks from ARC and GPQA confirm superior performance on structured reasoning datasets.  
      Early adopters report >30% improvement in multi-step correctness.
    assessment_notes: |
      Highly reliable for structured reasoning; not suitable for stylistic or open-domain tasks.
  safe:
    safety_measures: |
      Red-teamed for reasoning stability; refusal behaviors for unsafe autonomous reasoning chains.  
      Safety layer intercepts unsupervised code execution requests.
    known_safety_issues: |
      Verbose meta-reasoning may leak sensitive task context; unbounded step recursion risks cost overruns.
    assessment_notes: |
      Safe for supervised analytical tasks; requires governance for cost and recursion control.
  secure_and_resilient:
    security_features: |
      Enforced process-limiting at inference; compute-bounded reasoning steps.  
      Hosted on secure Azure OpenAI clusters (SOC 2 / FedRAMP High).
    known_vulnerabilities: |
      Long-chain reasoning may expose latent context injection paths; no multimodal hardening yet.
    assessment_notes: |
      Robust for controlled research use; sandbox tool execution mandatory.
  accountable_and_transparent:
    transparency_level: "Medium"
    auditability: |
      Detailed system card available; private CoT prevents full interpretability but reasoning process reproducible.
    assessment_notes: |
      Partial transparency; high operational reproducibility.
  explainable_and_interpretable:
    explainability_features: |
      Stepwise reasoning output summaries; optional "rationale compression" feature in API.  
    interpretability_limitations: |
      Internal reasoning chain remains private; external explanations post-hoc only.
    assessment_notes: |
      Explainability moderate; reliability compensates for interpretability opacity.
  privacy_enhanced:
    privacy_features: |
      PII filters and dataset audit logs; enterprise isolation enforced.  
    privacy_concerns: |
      None material; reasoning traces synthetic.
    assessment_notes: |
      Privacy compliant with OpenAI Enterprise and Teams governance policies.
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Limited generative output reduces exposure to harmful bias; 
      fairness evaluation performed on structured text only.
    known_biases: |
      Negligible in reasoning tasks; slight calibration bias in open-ended queries.
    assessment_notes: |
      Minimal fairness risk given domain-specific use.

# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    - Multi-step reasoning verification using domain-specific datasets  
    - Latency and token-cost profiling for long derivations  
    - Consistency validation on repeated complex problems  
    - Factual correctness audit in scientific domains
  key_evaluation_questions: |
    - Are reasoning outputs verifiable in your workflow?  
    - Do latency and token costs fit operational budgets?  
    - Does private CoT limit required audit visibility?
  comparison_considerations: |
    - Outperforms Claude 4.2 Opus and Gemini 1.5 Pro in reasoning;  
      slower and less conversational; unmatched process accuracy in 2025 class.

# # =============================================================================
# # [NIST AI RMF MAPPING]
# # Mapping of model risks to the RMF lifecycle functions (Govern, Map,
# # Measure, Manage).
# # =============================================================================
rmf_function_mapping:
  govern:
    notes: |
      Define explicit review checkpoints for reasoning-heavy deployments.  
      Require audit trail for derived conclusions.
  map:
    context_considerations: |
      Identify computation depth, reasoning verification needs, and cost ceilings.
    risk_categories: ["latency_overrun", "reasoning_error", "bias", "prompt_injection"]
  measure:
    suggested_metrics: |
      Stepwise correctness rate, hallucination frequency, token cost per reasoning chain, latency profile.
  manage:
    risk_management_considerations: |
      Enable process trace logging; set max reasoning depth; review accuracy metrics quarterly.

# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://openai.com/index/openai-o1/"
    description: "Official o1 announcement and technical overview"
  - url: "https://platform.openai.com/docs/models/o1"
    description: "API model documentation and reasoning guide"
  benchmarks:
  - name: "MMLU"
    url: "https://arxiv.org/abs/2509.01412"
    result: "94.1"
  - name: "GSM8K"
    url: "https://arxiv.org/abs/2509.01412"
    result: "99.0"
  third_party_evaluations:
  - source: "ARC Reasoning Benchmark (2025)"
    url: "https://arxiv.org/abs/2509.01412"
    summary: "Validated o1’s reasoning accuracy and process supervision benefits."
  news_coverage:
  - title: "OpenAI launches o1 — a reasoning-optimized model surpassing GPT-4o in logic"
    url: "https://openai.com/index/openai-o1/"
    date: "2025-09-12"

# # =============================================================================
# # [CARD METADATA]
# # Information about the model card document itself (author, version, history).
# # =============================================================================
metadata:
  card_version: "1.0"
  card_author: "NIST AI RMF Cookbook Team"
  card_creation_date: "2025-10-17"
  last_updated: "2025-10-17"
  information_sources: |
    OpenAI o1 release materials, ARC benchmark reports, and independent reasoning evaluations.
  completeness_assessment: |
    High for reasoning and benchmark data; medium for architectural transparency.  
  change_log:
  - date: "2025-10-17"
    author: "Cookbook Team"
    changes: "Initial card created from o1 release documentation and benchmark analysis."
