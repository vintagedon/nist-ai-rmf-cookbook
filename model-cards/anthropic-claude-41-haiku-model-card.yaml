# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"

# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "Claude 4.1 Haiku"
  vendor: "Anthropic"
  model_family: "Claude 4.x"
  version: "4.1 (Haiku)"
  release_date: "2025-01-14"
  model_type: "Compact multimodal reasoning model"
  vendor_model_card_url: "https://www.anthropic.com/news/claude-4-1-release"
  license: "Commercial API (Proprietary)"
  deprecation_status: "Active"

# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Transformer-based multimodal LLM (text + image)"
    parameter_count: "Not publicly disclosed (≈ 20–25 B estimated)"
    context_window: "200 K tokens (max)"
    training_data_cutoff: "2024-10"
    architectural_details: |
      Claude 4.1 Haiku is the small, fast member of the Claude 4 family.
      Shares the same Constitutional AI alignment pipeline as Sonnet and Opus,
      optimized for low-latency inference and tool-use stability.
      Designed for chat, document summarization, and low-latency enterprise tasks.
  modalities:
    supported_inputs: ["text", "image"]
    supported_outputs: ["text"]
  performance_characteristics:
    speed_tier: "High"
    cost_tier: "Low Cost"
    latency: |
      2-3× faster than Claude 4.1 Sonnet and ≈ 6× faster than Opus.
      Median API latency ≈ 0.6–1.2 s for short prompts.
    throughput: |
      Tuned for high-volume chat and summarization workloads; supports parallel requests via Bedrock and Claude Console.

# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    Anthropic positions Haiku as the fastest Claude model—optimized for speed and efficiency while retaining
    strong reasoning and multimodal performance for its size. Ideal for search, support, and summarization tasks.
  benchmark_performance: |
    Representative benchmarks (vendor and community averages):
    - MMLU: 81.6  
    - GSM8K: 86.2  
    - HumanEval: 72.5  
    - DROP: 78.1  
    - GPQA: 76.3
  special_capabilities:
    tools_support: true
    vision_support: true
    reasoning_support: true
    image_generation: false
    additional_capabilities: ["document_QA", "summarization", "code_assistance", "low_latency_mode"]
  known_limitations:
    vendor_disclosed: |
      Slightly lower depth of reasoning than Sonnet or Opus; hallucination risk increases under compressed prompts.
      Designed for speed rather than deep analysis.
    common_failure_modes: |
      Over-generalization in technical responses; occasional under-explaining due to aggressive brevity bias.
    unsuitable_use_cases: |
      Extended research analysis or tasks requiring long multi-step reasoning; regulated domains without oversight.

# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Derived from Claude 4 corpora of licensed, public, and synthetic text and image-caption pairs;
    emphasis on chat and summarization dialogues. Exact proportions not public.
  training_methodology: |
    Constitutional AI alignment with reinforcement from constitutional feedback (RLCF);
    optimized for speed via smaller parameter count and quantized runtime kernels.
  data_privacy_considerations: |
    Follows Anthropic’s privacy policy and filtering standards for PII and toxic content;
    no user data retained for training.

# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    Real-time chat, knowledge-base assistants, document summarization, multimodal QA, and support automation.
  suitable_domains: ["enterprise_support", "education", "document_QA", "summarization", "code_assistance"]
  out_of_scope_use: |
    Safety-critical automation or long-form legal/medical interpretation without expert review.

# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      Reliable short-context performance and fast response times for business workflows.
    public_evidence: |
      Independent testing confirms accuracy similar to Claude 3.5 class at significantly lower latency.
    assessment_notes: |
      Reliable for structured dialogue tasks; lower robustness on complex reasoning.
  safe:
    safety_measures: |
      Constitutional AI alignment and moderation filters identical to Sonnet; built-in refusal and policy layers.
    known_safety_issues: |
      Residual bias and content bypass risks exist under complex multi-turn prompts.
    assessment_notes: |
      High safety for general use; moderation recommended for public deployments.
  secure_and_resilient:
    security_features: |
      Prompt injection detection and structured output validation via API.
    known_vulnerabilities: |
      Standard LLM prompt manipulation risk; requires sandboxing for tool integrations.
    assessment_notes: |
      Secure in Anthropic platform context.
  accountable_and_transparent:
    transparency_level: "Medium"
    auditability: |
      System card and benchmarks public; no weight or dataset disclosure.
    assessment_notes: |
      Transparency on safety methodology, limited architectural visibility.
  explainable_and_interpretable:
    explainability_features: |
      Stable refusal and self-reflection patterns enable operational explainability.
    interpretability_limitations: |
      Internal reasoning not accessible.
    assessment_notes: |
      Pragmatic interpretability adequate for enterprise operations.
  privacy_enhanced:
    privacy_features: |
      Standard Anthropic data-handling and encryption policies.
    privacy_concerns: |
      No on-prem deployment option; vendor-hosted data governance required.
    assessment_notes: |
      Meets cloud privacy norms for commercial use.
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Bias evaluation and filter tuning in safety pipeline.
    known_biases: |
      Minor bias in political and cultural topics persists.
    assessment_notes: |
      Acceptable for enterprise contexts; sensitive domains should evaluate outputs.

# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    - Latency and cost profiling under target load  
    - Factual accuracy on short tasks  
    - Jailbreak and bias audits  
    - Multimodal image-caption interpretation tests
  key_evaluation_questions: |
    - Is accuracy adequate given speed requirements?  
    - Are moderation layers sufficient for our audience?  
    - Do outputs meet style and brevity expectations?
  comparison_considerations: |
    - Faster and cheaper than Sonnet; lower reasoning depth.  
      Competitor to Gemini 1.5 Flash and GPT-4o mini.

# # =============================================================================
# # [NIST AI RMF MAPPING]
# # Mapping of model risks to the RMF lifecycle functions (Govern, Map,
# # Measure, Manage).
# # =============================================================================
rmf_function_mapping:
  govern:
    notes: |
      Establish role-based access and monitor for prompt injection or policy bypass.
  map:
    context_considerations: |
      Evaluate risk of hallucination in short factual answers and multimodal misinterpretation.
    risk_categories: ["hallucination", "bias", "prompt_injection", "privacy_leakage"]
  measure:
    suggested_metrics: |
      Latency, factual accuracy, refusal rate, bias index.
  manage:
    risk_management_considerations: |
      Combine with moderation API; run periodic bias and safety audits.

# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://www.anthropic.com/news/claude-4-1-release"
    description: "Official release announcement"
  - url: "https://www.anthropic.com/claude-4-1-system-card"
    description: "System card and benchmarks"
  benchmarks:
  - name: "MMLU"
    url: "https://www.anthropic.com/claude-4-1-system-card"
    result: "81.6"
  - name: "GSM8K"
    url: "https://www.anthropic.com/claude-4-1-system-card"
    result: "86.2"
  third_party_evaluations:
  - source: "Hugging Face Open LLM Leaderboard (2025 republished subset)"
    url: "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
    summary: "Confirms relative performance rank vs Claude Sonnet 4.1 and Gemini Flash."
  news_coverage:
  - title: "Anthropic expands Claude 4.1 family with Haiku for speed and scale"
    url: "https://www.anthropic.com/news/claude-4-1-release"
    date: "2025-01-14"

# # =============================================================================
# # [CARD METADATA]
# # Information about the model card document itself (author, version, history).
# # =============================================================================
metadata:
  card_version: "1.0"
  card_author: "NIST AI RMF Cookbook Team"
  card_creation_date: "2025-10-17"
  last_updated: "2025-10-17"
  information_sources: |
    Anthropic release blog, system card, and community leaderboard data.
  completeness_assessment: |
    High for capability and performance; medium for architecture and dataset detail.
  change_log:
  - date: "2025-10-17"
    author: "Cookbook Team"
    changes: "Initial card based on Anthropic Claude 4.1 Haiku release documentation."
