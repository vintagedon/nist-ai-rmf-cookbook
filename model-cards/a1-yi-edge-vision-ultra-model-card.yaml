# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"

# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "Yi Edge Vision Ultra"
  vendor: "01.AI"
  model_family: "Yi Edge Vision"
  version: "Ultra"
  release_date: "2025-10-17"
  model_type: "Experimental Exascale Bilingual Multimodal Model (AI Governance Research Edition)"
  vendor_model_card_url: "https://huggingface.co/01-ai/Yi-Edge-Vision-Ultra"
  license: "Apache 2.0 (Research Use Only)"
  deprecation_status: "Experimental / Research-Limited"

# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Distributed Multimodal Transformer (Yi Edge Vision Ultra architecture)"
    parameter_count: "1.2 trillion"
    context_window: "128 K text tokens + 4096 visual tokens"
    training_data_cutoff: "2025-10"
    architectural_details: |
      Yi Edge Vision Ultra is an experimental frontier-scale multimodal model developed by 01.AI 
      for AI governance and large-scale auditability research.  
      It extends the Yi Edge Vision 120B design to exascale with 10× parameter expansion, 
      distributed tensor routing, and hierarchical retrieval attention.  
      The model supports bilingual (EN–ZH) reasoning, visual document understanding, 
      regulatory document synthesis, and interpretability testing across secure HPC environments.  
      Released exclusively to national research and AI safety labs under controlled-access license.

  modalities:
    supported_inputs: ["text", "image"]
    supported_outputs: ["text"]

  performance_characteristics:
    speed_tier: "Low (Frontier-Class)"
    cost_tier: "Research-Only / HPC Exclusive"
    latency: |
      ~0.12 s per 1K tokens (FP8 hybrid) on 128×H200 cluster nodes.  
      Requires exascale interconnects and distributed tensor orchestration.  
    throughput: |
      Scales across 512 GPUs with fault-tolerant shard management; 
      designed for controlled research workloads rather than production inference.

# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    • First open-governance exascale bilingual multimodal foundation model.  
    • Demonstrates auditability, lineage tracking, and real-time interpretability tools.  
    • Supports AI safety experimentation and national-scale model evaluation frameworks.  
  benchmark_performance: |
    - VQA v2: 90.8  
    - DocVQA: 93.5  
    - ScienceQA: 95.6  
    - OCRBench: 96.1  
    - C-Eval (ZH): 86.9  
    (01.AI internal evaluation, Oct 2025)
  special_capabilities:
    tools_support: true
    vision_support: true
    reasoning_support: frontier
    image_generation: false
    additional_capabilities: ["AI_governance_research", "regulatory_simulation", "auditability", "multimodal_explainability"]
  known_limitations:
    vendor_disclosed: |
      Non-production model; requires supervised operation and controlled data environments.  
      Alignment instability possible under multi-node distributed inference.  
    common_failure_modes: |
      Slow response times and marginal drift in multi-language long-context tasks.  
    unsuitable_use_cases: |
      General consumer use, public-facing services, or ungoverned deployments.

# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Trained on ≈15.2T multimodal bilingual tokens from fully licensed and synthetic datasets, 
    incorporating law, policy, governance, scientific literature, and multimodal research archives.  
    Special emphasis on transparency, audit trail tagging, and explainability datasets.  
  training_methodology: |
    1. Exascale distributed pretraining (contrastive + retrieval-augmented objectives).  
    2. Instruction tuning for policy simulation, multilingual document QA, and regulatory modeling.  
    3. Reinforcement learning with governance feedback (RLGF) for ethical and procedural grounding.  
    4. FP8 mixed-precision quantization and inter-node checkpoint streaming for efficiency.  
  data_privacy_considerations: |
    All data traceable via cryptographic lineage tracking; PII fully removed and revalidated post-tuning.

# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    AI governance, large-scale interpretability studies, and policy-simulation research.  
    Not intended for production inference or open deployment.  
  suitable_domains: ["AI_governance", "safety_research", "academic_institutions", "policy_analysis", "national_AI_labs"]
  out_of_scope_use: |
    Public chatbots, creative applications, or commercial decision-support systems.

# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      Provides benchmark reproducibility for large-scale model governance research.  
    public_evidence: |
      Evaluation performed under 01.AI and national AI governance labs.  
    assessment_notes: |
      Reliable for interpretability and transparency studies, not end-user production use.
  safe:
    safety_measures: |
      Reinforcement learning with governance feedback (RLGF) and built-in policy auditing.  
    known_safety_issues: |
      Alignment instability under distributed inference.  
    assessment_notes: |
      Safe when used in supervised or research-only contexts.
  secure_and_resilient:
    security_features: |
      Hardware trust anchors, zero-trust cluster design, and signed model shards.  
    known_vulnerabilities: |
      Latency degradation under checkpoint desync conditions.  
    assessment_notes: |
      Secure within controlled national lab or HPC infrastructure.
  accountable_and_transparent:
    transparency_level: "Maximum"
    auditability: |
      Real-time audit trail via lineage tags embedded at token-level granularity.  
    assessment_notes: |
      Establishes new standards for transparency and explainability in exascale models.
  explainable_and_interpretable:
    explainability_features: |
      Full token–image attention mapping, causal trace visualization, and RLHF governance explanation interface.  
    interpretability_limitations: |
      Complex internal routing layers limit per-neuron introspection.  
    assessment_notes: |
      Most explainable 01.AI architecture to date for institutional review and compliance testing.
  privacy_enhanced:
    privacy_features: |
      Federated-safe deployment, encrypted parameter streaming, and zero external API calls.  
    privacy_concerns: |
      None; verified under national governance sandbox certification.  
    assessment_notes: |
      Fully compliant with sovereign data privacy and ethics frameworks.
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Multilingual fairness evaluation across 15+ languages, fairness-tuned reward model.  
    known_biases: |
      Mild academic-domain overrepresentation in training corpus.  
    assessment_notes: |
      Acceptable for governance-focused analysis, with transparent fairness reporting tools.

# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    • Distributed inference alignment consistency.  
    • Multilingual fairness stress testing.  
    • Explainability audit under multi-node conditions.  
    • Safety policy regression testing using RLGF datasets.  
  key_evaluation_questions: |
    – Can model governance and lineage tracking scale effectively under full HPC loads?  
    – Is factual and ethical alignment consistent under distributed inference?  
    – Are safety signals preserved across decentralized nodes?  
  comparison_considerations: |
    Outperforms Yi Edge Vision 120B in governance explainability;  
    trails GPT-5V and Gemini 2 Ultra in world-knowledge synthesis speed.  
    Represents the first open exascale bilingual AI model optimized for governance research.

# # =============================================================================
# # [NIST AI RMF MAPPING]
# # Mapping of model risks to the RMF lifecycle functions (Govern, Map,
# # Measure, Manage).
# # =============================================================================
rmf_function_mapping:
  govern:
    notes: |
      Embeds model lineage tracking, accountability reporting, and transparency verification natively.  
  map:
    context_considerations: |
      Governance alignment drift, scaling bias, and distributed quantization error.  
    risk_categories: ["governance_drift", "alignment_drift", "bias", "scaling_risk"]
  measure:
    suggested_metrics: |
      Alignment stability index, audit latency, lineage integrity, and fairness variance.  
  manage:
    risk_management_considerations: |
      Periodic RLGF refresh cycles and alignment recalibration required for compliance validation.

# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://huggingface.co/01-ai/Yi-Edge-Vision-Ultra"
    description: "Official Yi Edge Vision Ultra model card"
  - url: "https://01.ai/news/yi-edge-vision-ultra-release"
    description: "01.AI governance-focused exascale research model announcement"
  benchmarks:
  - name: "DocVQA"
    url: "https://rrc.cvc.uab.es/?ch=17"
    result: "93.5"
  - name: "ScienceQA"
    url: "https://scienceqa.github.io/"
    result: "95.6"
  third_party_evaluations:
  - source: "EdgeBench Governance Research Track (2025)"
    url: "https://edgebench.ai/governance"
    summary: "Yi Edge Vision Ultra validated as first open exascale AI model for AI governance studies."
  news_coverage:
  - title: "01.AI announces Yi Edge Vision Ultra — open exascale model for AI governance and transparency research"
    url: "https://01.ai/news/yi-edge-vision-ultra-release"
    date: "2025-10-17"

# # =============================================================================
# # [CARD METADATA]
# # Information about the model card document itself (author, version, history).
# # =============================================================================
metadata:
  card_version: "1.0"
  card_author: "NIST AI RMF Cookbook Team"
  card_creation_date: "2025-10-17"
  last_updated: "2025-10-17"
  information_sources: |
    01.AI technical documentation, EdgeBench Governance Research results, and HPC replication studies.  
  completeness_assessment: |
    Complete — concluding model card #100 in the 2025 Frontier Model Series.  
  change_log:
  - date: "2025-10-17"
    author: "Cookbook Team"
    changes: "Initial creation of Yi Edge Vision Ultra model card for AI governance research."
