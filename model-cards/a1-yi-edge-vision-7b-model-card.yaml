# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"

# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "Yi Edge Vision 7B"
  vendor: "01.AI"
  model_family: "Yi Edge Vision"
  version: "7B"
  release_date: "2025-09-28"
  model_type: "Compact Bilingual Multimodal Model (Offline OCR and Document QA)"
  vendor_model_card_url: "https://huggingface.co/01-ai/Yi-Edge-Vision-7B"
  license: "Apache 2.0"
  deprecation_status: "Active"

# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Multimodal Transformer (Yi Edge 7B text + ViT-B/16 encoder)"
    parameter_count: "7 billion"
    context_window: "8 K text tokens + 512 visual tokens"
    training_data_cutoff: "2025-08"
    architectural_details: |
      Yi Edge Vision 7B is a lightweight multimodal bilingual model optimized for offline OCR, 
      document understanding, and visual question answering (VQA).  
      It integrates the Yi Edge 7B bilingual reasoning base with a compact Vision Transformer 
      (ViT-B/16) encoder for low-latency image–text fusion.  
      FP8 and INT4 quantization enable deployment on consumer laptops, IoT edge gateways, 
      and portable devices without reliance on cloud resources.

  modalities:
    supported_inputs: ["text", "image"]
    supported_outputs: ["text"]

  performance_characteristics:
    speed_tier: "Very High (Edge-Optimized Multimodal)"
    cost_tier: "Free / Open-weight"
    latency: |
      ~0.09 s per 1K text tokens + ~0.15 s per 224×224 image (INT4 RTX 4060).  
      Performs real-time OCR QA and visual reasoning on 8 GB VRAM devices.  
    throughput: |
      Designed for on-device document QA, translation, and visual context summarization.

# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    • Compact, efficient bilingual multimodal reasoning for OCR and document QA.  
    • Runs offline without cloud dependency or telemetry.  
    • High alignment between visual text content and multilingual reasoning.  
  benchmark_performance: |
    - VQA v2: 79.5  
    - DocVQA: 83.7  
    - ScienceQA (Text+Image): 85.2  
    - C-Eval (ZH): 74.6  
    - OCRBench: 89.4  
    (01.AI internal + EdgeBench Multimodal, Sep 2025)
  special_capabilities:
    tools_support: true
    vision_support: true
    reasoning_support: strong
    image_generation: false
    additional_capabilities: ["document_QA", "OCR_translation", "captioning", "multimodal_RAG"]
  known_limitations:
    vendor_disclosed: |
      Limited image size support (≤512×512).  
      Underperforms on abstract or diagram-heavy images.  
    common_failure_modes: |
      Occasional OCR hallucination under low-resolution or stylized fonts.  
    unsuitable_use_cases: |
      Continuous video analysis or large-scale batch image processing.

# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Trained on ≈2.3T multimodal bilingual tokens combining text, scanned documents, 
    and synthetic OCR data.  
    Key datasets: Wukong-Doc, LAION-COCO, ChartQA, and bilingual government document archives.  
    Data was preprocessed for PII removal, multilingual balance, and font diversity.
  training_methodology: |
    1. Multimodal pretraining using OCR–caption contrastive alignment.  
    2. Instruction fine-tuning for document QA and translation tasks.  
    3. Quantization-aware training (QAT) for edge deployment stability.  
    4. Bilingual DPO alignment for safety and factual reasoning.  
  data_privacy_considerations: |
    All training data publicly licensed or synthetic; sensitive document samples anonymized.

# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    Offline document comprehension, OCR QA, and bilingual reasoning for research, education, and enterprise.  
    Targeted for use in local RAG systems, legal document review, and academic archives.  
  suitable_domains: ["education", "enterprise_AI", "document_intelligence", "translation", "edge_AI"]
  out_of_scope_use: |
    Medical imaging, surveillance, or any human-identification use case.

# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      Consistent OCR QA performance across English and Chinese datasets.  
    public_evidence: |
      Verified on EdgeBench Multimodal and open Hugging Face evaluations.  
    assessment_notes: |
      Reliable for edge-scale OCR and document reasoning.
  safe:
    safety_measures: |
      Multilingual alignment and OCR content-filter tuning.  
    known_safety_issues: |
      Can misclassify or redact too aggressively on sensitive documents.  
    assessment_notes: |
      Safe under enterprise privacy and document-control policies.
  secure_and_resilient:
    security_features: |
      Telemetry-free operation, signed model weights, and offline integrity verification.  
    known_vulnerabilities: |
      Potential image prompt injection via crafted document metadata.  
    assessment_notes: |
      Secure for air-gapped or isolated environments.
  accountable_and_transparent:
    transparency_level: "High"
    auditability: |
      Dataset composition, OCR pipelines, and fine-tuning parameters documented.  
    assessment_notes: |
      Meets transparency standards for open multimodal models.
  explainable_and_interpretable:
    explainability_features: |
      Visual token attention heatmaps and multilingual grounding visualization.  
    interpretability_limitations: |
      Reduced cross-modal interpretability under heavy quantization.  
    assessment_notes: |
      Adequate explainability for enterprise or research deployments.
  privacy_enhanced:
    privacy_features: |
      PII scrubbing, on-device inference, and encrypted cache handling.  
    privacy_concerns: |
      None significant; compliant with offline privacy frameworks.  
    assessment_notes: |
      Meets edge privacy and compliance standards.
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Multilingual dataset balancing, cross-font fairness testing, and OCR neutrality filters.  
    known_biases: |
      Underrepresentation of low-resource languages and handwritten scripts.  
    assessment_notes: |
      Acceptable for research and enterprise environments.

# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    • OCR accuracy and document QA benchmarking.  
    • Bilingual parity testing (EN↔ZH).  
    • Edge device latency and quantization drift analysis.  
    • Fairness and bias audits on document types and languages.  
  key_evaluation_questions: |
    – Are OCR and reasoning outputs consistent across languages?  
    – Is latency acceptable for embedded or offline devices?  
    – Are document redaction and privacy filters functioning correctly?  
  comparison_considerations: |
    Outperforms MiniCPM-V 2 and Pixtral 7B on OCR QA;  
    trails Yi Lightning Vision 13B and Qwen-VL 72B in general visual reasoning.  
    Leading open bilingual edge multimodal OCR model in late 2025.

# # =============================================================================
# # [NIST AI RMF MAPPING]
# # Mapping of model risks to the RMF lifecycle functions (Govern, Map,
# # Measure, Manage).
# # =============================================================================
rmf_function_mapping:
  govern:
    notes: |
      Integrate multimodal data governance and PII filtering policies under NIST AI RMF "Govern."  
  map:
    context_considerations: |
      OCR hallucination, multilingual bias, and privacy leakage risks.  
    risk_categories: ["bias", "privacy", "hallucination", "quantization_drift"]
  measure:
    suggested_metrics: |
      OCR accuracy (WER), QA F1 score, bilingual fairness index, latency.  
  manage:
    risk_management_considerations: |
      Perform periodic OCR pipeline audits and quantization recalibration.

# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://huggingface.co/01-ai/Yi-Edge-Vision-7B"
    description: "Official Yi Edge Vision 7B model card"
  - url: "https://01.ai/news/yi-edge-vision7b-release"
    description: "01.AI release announcement and EdgeBench Multimodal benchmarks"
  benchmarks:
  - name: "DocVQA"
    url: "https://rrc.cvc.uab.es/?ch=17"
    result: "83.7"
  - name: "OCRBench"
    url: "https://ocrbench.ai/"
    result: "89.4"
  third_party_evaluations:
  - source: "EdgeBench Multimodal (2025)"
    url: "https://edgebench.ai/multimodal"
    summary: "Yi Edge Vision 7B validated as leading edge OCR reasoning model."
  news_coverage:
  - title: "01.AI releases Yi Edge Vision 7B — bilingual multimodal OCR model for offline AI"
    url: "https://01.ai/news/yi-edge-vision7b-release"
    date: "2025-09-28"

# # =============================================================================
# # [CARD METADATA]
# # Information about the model card document itself (author, version, history).
# # =============================================================================
metadata:
  card_version: "1.0"
  card_author: "NIST AI RMF Cookbook Team"
  card_creation_date: "2025-10-17"
  last_updated: "2025-10-17"
  information_sources: |
    01.AI Edge Vision technical docs, Hugging Face evaluations, and EdgeBench OCR benchmarks.  
  completeness_assessment: |
    Very high for transparency and performance data; moderate for low-resource OCR domain fairness.  
  change_log:
  - date: "2025-10-17"
    author: "Cookbook Team"
    changes: "Initial card created from Yi Edge Vision 7B release and benchmark documentation."
