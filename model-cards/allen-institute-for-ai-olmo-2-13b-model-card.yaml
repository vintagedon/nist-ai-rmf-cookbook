# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"

# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "OLMo 2 13B"
  vendor: "Allen Institute for AI (AI2)"
  model_family: "OLMo"
  version: "2 (13B)"
  release_date: "2025-03-10"
  model_type: "Transparent Open-Weight Reasoning and Alignment Model"
  vendor_model_card_url: "https://huggingface.co/allenai/OLMo-2-13B"
  license: "Apache 2.0"
  deprecation_status: "Active"

# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Dense Transformer (decoder-only)"
    parameter_count: "13 billion"
    context_window: "128 K tokens"
    training_data_cutoff: "2025-02"
    architectural_details: |
      OLMo 2 13B represents the Allen Institute for AI’s second-generation open model series,
      expanding the OLMo 7B line with larger scale, alignment fine-tuning, and long-context optimization.
      It was trained entirely on the Dolma v2 dataset (≈7T tokens) emphasizing transparent provenance,
      multilingual text, and safe alignment datasets for research reproducibility.
      Features grouped-query attention (GQA), rotary embeddings (RoPE),
      and FlashAttention 2 kernels for efficient long-context reasoning.

  modalities:
    supported_inputs: ["text"]
    supported_outputs: ["text"]

  performance_characteristics:
    speed_tier: "High"
    cost_tier: "Free / Open-weight"
    latency: |
      ~0.14 s per 1K tokens (fp16 A100), ~0.06 s INT4 quantized (RTX 4090).  
      Optimized for reproducibility, transparency, and alignment experiments.  
    throughput: |
      Efficient scaling across distributed clusters (up to 64 GPUs) with reproducible checkpoints.

# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    • Fully transparent long-context reasoning model.  
    • Open alignment fine-tuning process with bias and safety metrics published.  
    • Ideal for AI governance, reproducibility, and educational studies.  
  benchmark_performance: |
    - MMLU: 76.8  
    - GSM8K: 81.3  
    - ARC-C: 77.6  
    - TruthfulQA: 68.2  
    - HellaSwag: 80.5  
    (AI2 internal + Hugging Face Leaderboard, Apr 2025)
  special_capabilities:
    tools_support: true
    vision_support: false
    reasoning_support: true
    image_generation: false
    additional_capabilities: ["alignment_research", "bias_analysis", "long_context_reasoning"]
  known_limitations:
    vendor_disclosed: |
      Primarily English and Western-language focus; limited multilingual fine-tuning.  
      Unoptimized for creative or open-domain chat.  
    common_failure_modes: |
      Conservative completions and mild verbosity in factual QA.  
    unsuitable_use_cases: |
      Production customer support or unmoderated creative writing tasks.

# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Trained on Dolma v2, an open 7T-token dataset curated for transparency, fairness, and multilingual coverage.
    Includes academic, legal, scientific, and multilingual text with PII-filtering and bias audits.
  training_methodology: |
    Pretrained from scratch with open hyperparameters and public configuration files.  
    Followed by instruction-tuning and safety alignment using open datasets and reproducible RLHF pipelines.  
  data_privacy_considerations: |
    All data PII-filtered and governed by open dataset documentation; telemetry-free deployment.

# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    Research, AI alignment studies, bias auditing, transparency and reproducibility testing, and education.  
    Suitable for policy and governance research where open documentation is required.  
  suitable_domains: ["research", "education", "AI_governance", "alignment", "bias_studies"]
  out_of_scope_use: |
    Autonomous decision-making, commercial deployment, or non-reviewed creative applications.

# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      Consistent reasoning and high reproducibility for open alignment studies.  
    public_evidence: |
      Verified by AI2 open benchmark suite and external academic replications.  
    assessment_notes: |
      Reliable baseline for transparent AI research and alignment work.
  safe:
    safety_measures: |
      PII filtering, SafeRL alignment, and open red-teaming dataset.  
    known_safety_issues: |
      Limited cultural nuance beyond English; safe-mode refusals may underperform.  
    assessment_notes: |
      Safe and compliant for public research deployment.
  secure_and_resilient:
    security_features: |
      Deterministic builds, signed checkpoints, and public reproducibility pipeline.  
    known_vulnerabilities: |
      Standard open model risks: prompt-injection and malicious fine-tuning.  
    assessment_notes: |
      Secure for local or educational deployments.
  accountable_and_transparent:
    transparency_level: "Very High"
    auditability: |
      Full dataset lineage, training logs, and evaluation scripts released.  
    assessment_notes: |
      Exemplary transparency for governance and compliance benchmarking.
  explainable_and_interpretable:
    explainability_features: |
      Designed for neuron-level probing and interpretability experiments.  
    interpretability_limitations: |
      No integrated chain-of-thought supervision.  
    assessment_notes: |
      Ideal for interpretability and bias attribution research.
  privacy_enhanced:
    privacy_features: |
      Full PII filtering, Dolma open documentation, and telemetry-free model weights.  
    privacy_concerns: |
      None identified.  
    assessment_notes: |
      Meets privacy expectations for educational and public research.
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Multilingual dataset balancing, bias audits, and fairness calibration published with model.  
    known_biases: |
      Western-language dominance; mild academic dataset skew.  
    assessment_notes: |
      Strong open bias management framework.

# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    • Alignment and bias audits.  
    • Reproducibility verification with AI2 open scripts.  
    • Factual QA and safety benchmarking.  
    • Long-context stability evaluation (up to 128K tokens).  
  key_evaluation_questions: |
    – Do open lineage and bias audits meet governance criteria?  
    – Are model outputs stable under controlled evaluation conditions?  
    – Are interpretability hooks correctly implemented?  
  comparison_considerations: |
    Outperforms OLMo 7B in reasoning and alignment stability;  
    trails Qwen 2 72B and Phi-3 Medium 14B in domain versatility.  
    Flagship open governance model of early 2025.

# # =============================================================================
# # [NIST AI RMF MAPPING]
# # Mapping of model risks to the RMF lifecycle functions (Govern, Map,
# # Measure, Manage).
# # =============================================================================
rmf_function_mapping:
  govern:
    notes: |
      Supports open governance and transparency demonstration per NIST AI RMF "Govern" function.  
  map:
    context_considerations: |
      Identify transparency, bias, and reproducibility as key risk categories.  
    risk_categories: ["bias", "hallucination", "alignment_drift", "open_data_reuse"]
  measure:
    suggested_metrics: |
      Transparency index, reproducibility score, alignment deviation rate.  
  manage:
    risk_management_considerations: |
      Conduct regular fairness, reproducibility, and bias verification cycles.

# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://huggingface.co/allenai/OLMo-2-13B"
    description: "Official OLMo 2 13B model card and documentation"
  - url: "https://ai2-olmo.allenai.org/"
    description: "Allen AI OLMo project homepage and transparency portal"
  benchmarks:
  - name: "MMLU"
    url: "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
    result: "76.8"
  - name: "GSM8K"
    url: "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
    result: "81.3"
  third_party_evaluations:
  - source: "Hugging Face Open LLM Leaderboard (2025)"
    url: "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
    summary: "OLMo 2 13B benchmarked as top-tier open alignment and transparency model."
  news_coverage:
  - title: "AI2 releases OLMo 2 — open alignment-ready model for AI governance research"
    url: "https://allenai.org/news/olmo2-release"
    date: "2025-03-10"

# # =============================================================================
# # [CARD METADATA]
# # Information about the model card document itself (author, version, history).
# # =============================================================================
metadata:
  card_version: "1.0"
  card_author: "NIST AI RMF Cookbook Team"
  card_creation_date: "2025-10-17"
  last_updated: "2025-10-17"
  information_sources: |
    AI2 OLMo documentation, Dolma v2 dataset records, academic evaluations, Hugging Face benchmarks.  
  completeness_assessment: |
    Very high for transparency, bias management, and governance readiness; medium for multilingual benchmarking.  
  change_log:
  - date: "2025-10-17"
    author: "Cookbook Team"
    changes: "Initial card created from OLMo 2 13B release and benchmark data."
