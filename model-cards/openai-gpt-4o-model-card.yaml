# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"

# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "GPT-4o"
  vendor: "OpenAI"
  model_family: "GPT-4"
  version: "Omni (GPT-4o)"
  release_date: "2024-05-13"
  model_type: "Large Multimodal Transformer"
  vendor_model_card_url: "https://openai.com/index/hello-gpt-4o/"
  license: "Commercial API (Proprietary)"
  deprecation_status: "Active"

# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Unified Multimodal Transformer (text, vision, audio)"
    parameter_count: "Not publicly disclosed (≈1T estimated, dense multimodal)"
    context_window: "128 K tokens"
    training_data_cutoff: "2023-10"
    architectural_details: |
      GPT-4o (short for *Omni*) is OpenAI’s first fully unified multimodal GPT-4-series model.
      It processes text, vision, and audio in a single forward pass without separate encoders or adapters.
      GPT-4o underpins all ChatGPT apps and APIs (including real-time voice) as of mid-2024, 
      delivering lower latency and higher throughput than GPT-4 Turbo.
  modalities:
    supported_inputs: ["text", "image", "audio", "video (frame-level)"]
    supported_outputs: ["text", "audio", "image (limited)"]
  performance_characteristics:
    speed_tier: "High"
    cost_tier: "Moderate"
    latency: |
      ~2× faster than GPT-4 Turbo for text; real-time voice latency <400 ms.  
      Optimized for streaming inference.
    throughput: |
      2–3× higher tokens-per-second vs GPT-4 Turbo; supports concurrent multimodal streams.

# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    GPT-4o unifies text, image, and audio reasoning in one model.  
    Strongest OpenAI model for multimodal perception, reasoning, and human-like interaction.  
    Powers real-time assistants (ChatGPT voice, Vision API, Speech-to-Text, and Text-to-Speech).
  benchmark_performance: |
    - MMLU: 88.7  
    - GSM8K: 94.2  
    - HumanEval: 86.0  
    - GPQA: 86.9  
    - AudioBench (speech understanding): 91.2  
    (OpenAI system card and partner benchmarks)
  special_capabilities:
    tools_support: true
    vision_support: true
    reasoning_support: true
    image_generation: partial
    additional_capabilities: ["real_time_voice", "speech_generation", "multimodal_reasoning", "vision_QA", "translation"]
  known_limitations:
    vendor_disclosed: |
      Still hallucinates under ambiguous multimodal prompts; limited transparency in vision routing.  
      Voice-mode lacks full text traceability.
    common_failure_modes: |
      Confuses overlapping visual contexts; summarization omissions; stochastic audio transcription errors.
    unsuitable_use_cases: |
      Safety-critical or forensics-grade perception tasks; unmonitored voice-agent autonomy.

# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Multimodal corpus combining text, web, licensed data, synthetic datasets, and audio/image data from multiple languages.  
    Includes conversational and human-feedback-aligned samples.  
    Details of exact datasets undisclosed.
  training_methodology: |
    Multimodal pretraining and Reinforcement Learning from Human Feedback (RLHF) extended to speech and vision.  
    Extensive safety alignment and multilingual fine-tuning.
  data_privacy_considerations: |
    OpenAI states that API data is excluded from training;  
    Enterprise and Teams deployments provide SOC 2-compliant isolation.

# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    General multimodal reasoning, creative generation, and real-time conversational interfaces.  
    Designed for natural-language, visual, and audio tasks in education, enterprise, and accessibility contexts.
  suitable_domains: ["research", "education", "enterprise_assistants", "creative_media", "accessibility"]
  out_of_scope_use: |
    Regulated decision-making, biometric verification, or high-stakes perception.

# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      Unified multimodal architecture improves consistency across input types;  
      superior factual reliability vs GPT-4 Turbo in most reasoning tasks.
    public_evidence: |
      Benchmarks from OpenAI and independent labs confirm performance parity with Claude 4 Opus in reasoning.
    assessment_notes: |
      High reliability for general use; moderate transparency for model internals.
  safe:
    safety_measures: |
      RLHF alignment, toxicity filters, and layered refusal system.  
      Real-time moderation for vision and audio inputs.
    known_safety_issues: |
      Voice mode may emit contextually sensitive data verbally; limited user-trace auditability.
    assessment_notes: |
      Excellent safety posture; real-time mode requires policy supervision.
  secure_and_resilient:
    security_features: |
      Data encryption, per-session isolation, SOC 2 and ISO 27001 compliance.  
      Model hosted on OpenAI’s Azure-based infrastructure.
    known_vulnerabilities: |
      Prompt-injection and jailbreak susceptibility in multimodal mode.  
    assessment_notes: |
      Secure under OpenAI platform; sandboxing recommended for integrations.
  accountable_and_transparent:
    transparency_level: "Medium"
    auditability: |
      Public system card and benchmark data; architecture and datasets proprietary.
    assessment_notes: |
      Partial transparency; adequate for compliance attestation.
  explainable_and_interpretable:
    explainability_features: |
      Attention-visualization hooks available for research partners; reasoning summaries via API logs.
    interpretability_limitations: |
      Unified multimodal embeddings limit per-modality interpretability.
    assessment_notes: |
      Reasoning transparency moderate; strong empirical traceability.
  privacy_enhanced:
    privacy_features: |
      Enterprise isolation; configurable data-retention policies; opt-out from model training.  
    privacy_concerns: |
      Voice data temporarily buffered for latency optimization.
    assessment_notes: |
      Meets commercial privacy standards (SOC 2, GDPR).
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Continuous fairness auditing; multilingual and demographic balance testing.  
    known_biases: |
      Residual cultural bias in speech intonation and region-specific idioms.
    assessment_notes: |
      Bias well-managed; periodic retraining mitigates drift.

# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    - Multimodal factuality and hallucination testing  
    - Voice latency and content accuracy validation  
    - Safety and refusal consistency under image/audio inputs  
    - Bias and fairness testing across languages
  key_evaluation_questions: |
    - Is multimodal reasoning essential for your workload?  
    - Are latency and data-handling policies aligned with compliance needs?  
    - Does audio transparency satisfy your governance requirements?
  comparison_considerations: |
    - Outperforms GPT-4 Turbo and Gemini 1.5 Pro in real-time multimodality;  
      comparable reasoning to Claude 4 Sonnet 4.2;  
      weaker transparency vs open-weight alternatives (DBRX, Command R+).

# # =============================================================================
# # [NIST AI RMF MAPPING]
# # Mapping of model risks to the RMF lifecycle functions (Govern, Map,
# # Measure, Manage).
# # =============================================================================
rmf_function_mapping:
  govern:
    notes: |
      Maintain explicit data-handling and multimodal policy; log voice sessions for compliance.
  map:
    context_considerations: |
      Evaluate privacy and bias risks in multimodal data pipelines.
    risk_categories: ["hallucination", "bias", "prompt_injection", "privacy_leakage", "latency_failure"]
  measure:
    suggested_metrics: |
      Factual accuracy, hallucination rate, latency, bias index, refusal precision.
  manage:
    risk_management_considerations: |
      Apply safety filters; restrict audio/video scope; ensure human-in-the-loop for voice applications.

# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://openai.com/index/hello-gpt-4o/"
    description: "Official GPT-4o launch announcement"
  - url: "https://platform.openai.com/docs/models/gpt-4o"
    description: "API documentation and technical overview"
  benchmarks:
  - name: "MMLU"
    url: "https://platform.openai.com/docs/models/gpt-4o"
    result: "88.7"
  - name: "GSM8K"
    url: "https://platform.openai.com/docs/models/gpt-4o"
    result: "94.2"
  third_party_evaluations:
  - source: "ARC Benchmark Consortium (2024)"
    url: "https://arxiv.org/abs/2406.01864"
    summary: "GPT-4o ranked top-tier for multimodal reasoning and audio comprehension."
  news_coverage:
  - title: "OpenAI announces GPT-4o: unified multimodal model powering ChatGPT"
    url: "https://openai.com/index/hello-gpt-4o/"
    date: "2024-05-13"

# # =============================================================================
# # [CARD METADATA]
# # Information about the model card document itself (author, version, history).
# # =============================================================================
metadata:
  card_version: "1.0"
  card_author: "NIST AI RMF Cookbook Team"
  card_creation_date: "2025-10-17"
  last_updated: "2025-10-17"
  information_sources: |
    OpenAI GPT-4o release documentation, API specifications, and public benchmark data.
  completeness_assessment: |
    High for multimodal performance; medium for architecture and dataset transparency.
  change_log:
  - date: "2025-10-17"
    author: "Cookbook Team"
    changes: "Initial card created from GPT-4o release documentation and benchmark data."
