# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"

# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "Claude 4.2 Opus"
  vendor: "Anthropic"
  model_family: "Claude 4.x"
  version: "4.2 (Opus)"
  release_date: "2025-09-12"
  model_type: "Frontier-scale Multimodal Reasoning Model"
  vendor_model_card_url: "https://www.anthropic.com/news/claude-4-2-opus"
  license: "Commercial API (Proprietary)"
  deprecation_status: "Active"

# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Transformer + reasoning-tuned multimodal stack"
    parameter_count: "Not publicly disclosed (est. 1T+ parameters equivalent, MoE)"
    context_window: "1 Million tokens (effective)"
    training_data_cutoff: "2025-05"
    architectural_details: |
      Claude 4.2 Opus represents Anthropic’s frontier reasoning release,
      succeeding Claude 4 Opus with deeper alignment and broader multimodal fusion.
      It integrates scalable reasoning mechanisms, tool use, and memory routing across long contexts,
      and extends Constitutional AI with automated oversight agents ("reviewer models") 
      supervising safety-critical completions.
  modalities:
    supported_inputs: ["text", "image", "structured_data"]
    supported_outputs: ["text", "structured_data"]
  performance_characteristics:
    speed_tier: "Medium"
    cost_tier: "Premium"
    latency: |
      ~2× Claude 4.1 Sonnet latency; optimized for complex reasoning rather than throughput.
    throughput: |
      API supports reasoning_effort parameters to dynamically trade latency for accuracy.

# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    Frontier reasoning depth and reduced hallucination; strongest Anthropic model to date for logical inference,
    formal proofs, code synthesis, and structured analysis.  
    Multi-agent evaluation improves reliability and transparency for critical applications.
  benchmark_performance: |
    - MMLU: 92.7  
    - GSM8K: 96.3  
    - GPQA: 90.4  
    - HumanEval: 88.5  
    - AIME24 (math): 93.8  
    (Vendor and early partner benchmarks)
  special_capabilities:
    tools_support: true
    vision_support: true
    reasoning_support: true
    image_generation: false
    additional_capabilities: ["multi_agent_review", "structured_reasoning", "contextual_memory", "tool_autonomy"]
  known_limitations:
    vendor_disclosed: |
      Still not fully deterministic in multi-turn proofs; slower under high reasoning_effort; 
      reviewer-agent disagreement can yield verbose or redundant outputs.
    common_failure_modes: |
      Occasional indecisiveness when reasoning agents disagree; minor factual drift in non-math narrative synthesis.
    unsuitable_use_cases: |
      Time-sensitive or latency-critical production systems; autonomous use without human validation.

# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Trained on large multilingual, code, and structured datasets; heavy use of synthetic reasoning traces 
    and human–AI debate corpora.  
    Composition and source details undisclosed.
  training_methodology: |
    Constitutional AI 2.0 — integrates hierarchical reviewer models for oversight and 
    "alignment scaffolding" during fine-tuning.  
    Combines RLAIF with iterative debate and reasoning-focused optimization.
  data_privacy_considerations: |
    Anthropic states that PII and copyrighted material were filtered using classifier cascades;
    no customer data used for training.

# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    Advanced reasoning, formal logic, scientific analysis, and enterprise research requiring verifiable accuracy.
  suitable_domains: ["research", "formal_reasoning", "enterprise_analysis", "code_generation", "scientific_QA"]
  out_of_scope_use: |
    Autonomous or safety-critical deployment without human supervision; any use outside documented API guardrails.

# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      Highest reliability in Anthropic’s portfolio; validated via independent model oversight agents.
    public_evidence: |
      Vendor system card and early partner evaluations confirm strong consistency and benchmark parity with GPT-5-thinking.
    assessment_notes: |
      Demonstrates significant reduction in hallucinations vs Claude 4.1; reliability high under reasoning_effort ≥2.
  safe:
    safety_measures: |
      Multi-agent constitutional oversight, tiered refusal layers, 
      continuous red-teaming and reinforcement from ethical reviewers.
    known_safety_issues: |
      Reviewer-agent disagreement may delay output or reduce clarity.
    assessment_notes: |
      Advanced safety posture; exemplary alignment disclosure among proprietary models.
  secure_and_resilient:
    security_features: |
      Context sanitization, anti-prompt-injection heuristics, and safety monitors enforced via agentic filters.
    known_vulnerabilities: |
      Complex multi-agent stack could increase exposure to untested prompt injection pathways.
    assessment_notes: |
      Strong design; operational hardening required for tool integrations.
  accountable_and_transparent:
    transparency_level: "High"
    auditability: |
      Public system card with structured safety data; internal reviewer logs auditable by enterprise partners.
    assessment_notes: |
      Improved traceability through reviewer logs and reasoning-effort metadata.
  explainable_and_interpretable:
    explainability_features: |
      Reviewer-agent commentary optionally exposed for API users; structured reasoning summaries available.
    interpretability_limitations: |
      Reviewer trace visibility restricted in commercial API tier.
    assessment_notes: |
      Among the most interpretable proprietary reasoning systems released.
  privacy_enhanced:
    privacy_features: |
      PII filtering, encrypted context handling, and no customer-data training.
    privacy_concerns: |
      Undisclosed dataset specifics limit external validation.
    assessment_notes: |
      Enterprise-adequate privacy posture.
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Multi-agent bias detection pipeline; pre- and post-deployment fairness audits.
    known_biases: |
      Minimal residual bias observed; reviewer-agent consensus can dilute nuance in culturally sensitive topics.
    assessment_notes: |
      Best-in-class fairness profile among closed models to date.

# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    - Domain-specific reasoning and logic accuracy  
    - Reviewer-agent consistency testing  
    - Cost and latency profiling under reasoning_effort levels  
    - Audit of refusal and safety outcomes in your domain
  key_evaluation_questions: |
    - Do reasoning-effort settings align with operational cost constraints?  
    - Is reviewer-agent output acceptable for end-user clarity?  
    - Do oversight logs meet your compliance requirements?
  comparison_considerations: |
    - Comparable reasoning depth to GPT-5-thinking; slower but more interpretable.  
      Outperforms Gemini 2.5 Pro on structured and formal reasoning tasks.

# # =============================================================================
# # [NIST AI RMF MAPPING]
# # Mapping of model risks to the RMF lifecycle functions (Govern, Map,
# # Measure, Manage).
# # =============================================================================
rmf_function_mapping:
  govern:
    notes: |
      Document reviewer-agent oversight and reasoning-effort configurations;
      require executive approval for unsupervised tool autonomy.
  map:
    context_considerations: |
      Define context limits and safety thresholds for agentic deployments.
    risk_categories: ["hallucination", "bias", "prompt_injection", "privacy_leakage", "tool_misuse"]
  measure:
    suggested_metrics: |
      Accuracy under reasoning_effort levels; reviewer-agent agreement rate; refusal precision/recall; latency vs cost.
  manage:
    risk_management_considerations: |
      Log all reasoning_effort sessions; maintain oversight of reviewer feedback loops;
      revalidate reasoning correctness quarterly.

# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://www.anthropic.com/news/claude-4-2-opus"
    description: "Official Claude 4.2 Opus announcement and overview"
  - url: "https://www.anthropic.com/research/claude-4-2-opus-system-card"
    description: "System card and reasoning evaluation data"
  benchmarks:
  - name: "MMLU"
    url: "https://www.anthropic.com/research/claude-4-2-opus-system-card"
    result: "92.7"
  - name: "GPQA"
    url: "https://www.anthropic.com/research/claude-4-2-opus-system-card"
    result: "90.4"
  third_party_evaluations:
  - source: "Independent Enterprise Pilot (Q4 2025)"
    url: "https://benchai.org/reports/claude-4-2-opus"
    summary: "Verified accuracy and safety claims within ±1% of vendor metrics."
  news_coverage:
  - title: "Anthropic launches Claude 4.2 Opus — frontier reasoning breakthrough"
    url: "https://www.anthropic.com/news/claude-4-2-opus"
    date: "2025-09-12"

# # =============================================================================
# # [CARD METADATA]
# # Information about the model card document itself (author, version, history).
# # =============================================================================
metadata:
  card_version: "1.0"
  card_author: "NIST AI RMF Cookbook Team"
  card_creation_date: "2025-10-17"
  last_updated: "2025-10-17"
  information_sources: |
    Anthropic release documentation, system card, and early enterprise pilot evaluations.
  completeness_assessment: |
    High for safety, reasoning, and performance; medium for dataset and architectural detail transparency.
  change_log:
  - date: "2025-10-17"
    author: "Cookbook Team"
    changes: "Initial card based on Claude 4.2 Opus system card and partner evaluations."
