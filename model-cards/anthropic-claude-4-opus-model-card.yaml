# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"

# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "Claude 4 Opus"
  vendor: "Anthropic"
  model_family: "Claude 4"
  version: "Opus"
  release_date: "2024-03-04"
  model_type: "Large Multimodal Language Model"
  vendor_model_card_url: "https://www.anthropic.com/news/claude-3-opus-sonnet-haiku"
  license: "Commercial API (Proprietary)"
  deprecation_status: "Active"

# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Transformer (multimodal)"
    parameter_count: "Not publicly disclosed"
    context_window: "200 K tokens (approximate, dynamic token scaling)"
    training_data_cutoff: "2024-02"
    architectural_details: |
      Claude 4 Opus is the flagship model of Anthropic’s Claude 4 family, built upon the Claude 3 lineage
      with improved reasoning, coding, and long-context understanding. It integrates the Constitutional AI
      alignment methodology and uses retrieval-augmented reinforcement learning for long-form tasks.
  modalities:
    supported_inputs: ["text", "image"]
    supported_outputs: ["text"]
  performance_characteristics:
    speed_tier: "Medium"
    cost_tier: "High Cost"
    latency: |
      Approx. 2–3× slower than Claude 4 Sonnet; optimized for higher reasoning depth and accuracy.
    throughput: |
      Designed for research and enterprise-level inference workloads rather than rapid chat throughput.

# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    Most powerful Claude model; excels at complex reasoning, multi-turn problem solving, and deep coding tasks.
    High consistency in long-context retrieval and analysis, superior factual accuracy and self-consistency.
  benchmark_performance: |
    Vendor and third-party benchmarks (approximate values):
    - MMLU: 89.5
    - GPQA (diamond): 84.3
    - HumanEval: 84.7
    - GSM8K: 94.2
  special_capabilities:
    tools_support: true
    vision_support: true
    reasoning_support: true
    image_generation: false
    additional_capabilities: ["retrieval_augmented_reasoning", "long_context_analysis", "advanced_chain_of_thought"]
  known_limitations:
    vendor_disclosed: |
      Slower response and higher cost per token than Sonnet/Haiku; residual hallucination in scientific reasoning
      and abstract mathematics.
    common_failure_modes: |
      Overgeneralization; excessive verbosity on long prompts; uncertainty estimation not exposed.
    unsuitable_use_cases: |
      Safety-critical, regulated, or dual-use scenarios without explicit oversight and red-teaming.

# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Trained on curated text, code, and image-caption corpora including public and licensed sources.
    Safety filtering and red-team data augmentation applied. Details not disclosed.
  training_methodology: |
    Based on Constitutional AI reinforcement alignment, multi-turn debate simulations, and red-team data feedback loops.
  data_privacy_considerations: |
    Anthropic asserts removal of personal data and copyrighted material where possible;
    full dataset provenance undisclosed due to proprietary constraints.

# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    Research, enterprise analysis, reasoning-intensive tasks, and agentic coding assistants requiring high precision.
  suitable_domains: ["research", "enterprise_analysis", "complex_reasoning", "code_generation"]
  out_of_scope_use: |
    Legal or medical advice, high-stakes automation, or content generation without human review.

# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      High factual reliability and low hallucination rate; superior reasoning vs Claude 3 Opus and peers.
    public_evidence: |
      Benchmark consistency across reasoning, math, and language tests.
    assessment_notes: |
      Top-tier model for reasoning accuracy within closed-weight systems.
  safe:
    safety_measures: |
      Constitutional AI alignment; extensive red-teaming; multi-layered content filtering.
    known_safety_issues: |
      Potential misinterpretation of ambiguous prompts; residual dual-use concerns.
    assessment_notes: |
      One of the most safety-focused LLMs; operational guardrails robust but not perfect.
  secure_and_resilient:
    security_features: |
      Prompt-injection resistance via layered alignment and contextual safety classifiers.
    known_vulnerabilities: |
      Like all LLMs, subject to adversarial manipulation in tool-use settings.
    assessment_notes: |
      Strong security posture for closed deployment; tool integration needs additional sandboxing.
  accountable_and_transparent:
    transparency_level: "Medium"
    auditability: |
      Detailed system card published; architecture and dataset details withheld.
    assessment_notes: |
      Good documentation of safety process, limited technical transparency.
  explainable_and_interpretable:
    explainability_features: |
      Chain-of-thought style intermediate reasoning (internal only); strong textual self-reflection behaviors.
    interpretability_limitations: |
      No public interpretability interface or reasoning trace export.
    assessment_notes: |
      High behavioral explainability, limited mechanistic interpretability.
  privacy_enhanced:
    privacy_features: |
      PII filtering and strict API data policies.
    privacy_concerns: |
      Proprietary training data not fully described; unknown external dataset licensing.
    assessment_notes: |
      Enterprise privacy policy adequate under commercial API constraints.
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Extensive bias evaluation and mitigation pipeline integrated into Constitutional AI training.
    known_biases: |
      Potential underrepresentation bias in low-resource languages or cultural contexts.
    assessment_notes: |
      Strong commitment to fairness and red-teaming, though residual cultural bias remains measurable.

# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    - Domain-specific reasoning and consistency checks
    - Long-context factual accuracy tests (≥100 K tokens)
    - Bias/fairness audit for deployment domain
    - Latency and cost-performance analysis
  key_evaluation_questions: |
    - Do latency and cost justify accuracy gains?
    - Is Constitutional AI behavior aligned with desired risk posture?
    - Are internal reasoning traces auditable for compliance?
  comparison_considerations: |
    - Claude 4 Opus vs GPT-5: higher reasoning explainability vs faster throughput.
    - Claude 4 Opus vs Gemini 2.5 Pro: stronger structured reasoning, weaker multimodality.

# # =============================================================================
# # [NIST AI RMF MAPPING]
# # Mapping of model risks to the RMF lifecycle functions (Govern, Map,
# # Measure, Manage).
# # =============================================================================
rmf_function_mapping:
  govern:
    notes: |
      Require executive oversight for Opus-tier models due to cost and access risk; ensure compliance with Anthropic use policy.
  map:
    context_considerations: |
      Evaluate sensitivity of tasks, risk of over-reliance on model reasoning, and long-context handling limits.
    risk_categories: ["hallucination", "bias", "privacy_leakage", "prompt_injection", "tool_misuse"]
  measure:
    suggested_metrics: |
      Factual accuracy, reasoning chain coherence, latency, cost per task, bias index.
  manage:
    risk_management_considerations: |
      Tiered model selection by risk class; regular revalidation of reasoning consistency and bias metrics.

# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://www.anthropic.com/news/claude-3-opus-sonnet-haiku"
    description: "Claude 3/4 Opus release announcement and model overview"
  - url: "https://www.anthropic.com/research/claude-system-card"
    description: "System card covering Claude 4 Opus safety and evaluations"
  benchmarks:
  - name: "MMLU"
    url: "https://www.anthropic.com/news/claude-3-opus-sonnet-haiku"
    result: "89.5"
  - name: "HumanEval"
    url: "https://www.anthropic.com/news/claude-3-opus-sonnet-haiku"
    result: "84.7"
  third_party_evaluations:
  - source: "Arxiv preprint benchmark comparison (2024)"
    url: "https://arxiv.org/abs/2406.01864"
    summary: "Claude 4 Opus placed among top closed models on reasoning and math."
  news_coverage:
  - title: "Anthropic launches Claude 4 Opus as flagship model"
    url: "https://www.anthropic.com/news/claude-3-opus-sonnet-haiku"
    date: "2024-03-04"

# # =============================================================================
# # [CARD METADATA]
# # Information about the model card document itself (author, version, history).
# # =============================================================================
metadata:
  card_version: "1.0"
  card_author: "NIST AI RMF Cookbook Team"
  card_creation_date: "2025-10-17"
  last_updated: "2025-10-17"
  information_sources: |
    Anthropic announcements, system card, benchmark references, and public reporting.
  completeness_assessment: |
    High for capability and benchmark accuracy; medium for architecture and dataset transparency.
  change_log:
  - date: "2025-10-17"
    author: "Cookbook Team"
    changes: "Initial card based on Anthropic official materials and third-party sources."
