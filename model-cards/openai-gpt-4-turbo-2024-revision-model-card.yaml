# =============================================================================
# (c) 2025-10-25, v1.0
# Author: Don Fountain (https://github.com/vintagedon/)
# Repository: https://github.com/vintagedon/nist-ai-rmf-cookbook
#
# This model card is part of the NIST AI RMF Cookbook project, a community
# resource for AI risk management.
# =============================================================================

# yaml-language-server: $schema=./schemas/model-card.schema.yaml
schema_version: "1.0.0"

# # =============================================================================
# # [MODEL IDENTITY]
# # Core details about the model's name, vendor, version, and license.
# # =============================================================================
model_identity:
  name: "GPT-4 Turbo (2024)"
  vendor: "OpenAI"
  model_family: "GPT-4"
  version: "Turbo (April 2024 update)"
  release_date: "2024-04-09"
  model_type: "Large Language Model (text + limited image understanding)"
  vendor_model_card_url: "https://openai.com/index/gpt-4-turbo/"
  license: "Commercial API (Proprietary)"
  deprecation_status: "Supported (partially replaced by GPT-4o)"

# # =============================================================================
# # [TECHNICAL SPECIFICATIONS]
# # Details on the model's architecture, parameters, modalities, and performance.
# # =============================================================================
technical_specifications:
  architecture:
    base_architecture: "Transformer (decoder-only, multimodal adapter architecture)"
    parameter_count: "Not publicly disclosed (~1T parameters equivalent, sparse routing)"
    context_window: "128 K tokens"
    training_data_cutoff: "2023-12"
    architectural_details: |
      GPT-4 Turbo was OpenAI’s high-efficiency variant of GPT-4, 
      optimized for longer contexts, lower latency, and reduced inference cost.  
      Supports multimodal text + image input but outputs text only.  
      The April 2024 revision introduced major improvements to reasoning consistency and API throughput.  
      Served as the foundation for the GPT-4o multimodal unification.
  modalities:
    supported_inputs: ["text", "image"]
    supported_outputs: ["text"]
  performance_characteristics:
    speed_tier: "High"
    cost_tier: "Medium"
    latency: |
      Average latency 1–2 seconds for text, 2–3 seconds for multimodal tasks.  
      ~2× faster than GPT-4 (2023) baseline with 50% cost reduction.
    throughput: |
      Supports high concurrency and streaming responses; optimized for API and enterprise integrations.

# # =============================================================================
# # [CAPABILITIES & LIMITATIONS]
# # What the model can do, its performance, known weaknesses, and failure modes.
# # =============================================================================
capabilities:
  vendor_claimed_strengths: |
    Enhanced reasoning performance and long-context retention with improved efficiency.  
    Balanced accuracy and cost for enterprise-scale workloads.  
    Capable of handling multimodal inputs and extended code or document synthesis.
  benchmark_performance: |
    - MMLU: 87.5  
    - GSM8K: 93.8  
    - HumanEval: 84.7  
    - GPQA: 85.5  
    - HellaSwag: 87.2  
    (OpenAI and ARC benchmark data, 2024)
  special_capabilities:
    tools_support: true
    vision_support: true
    reasoning_support: true
    image_generation: false
    additional_capabilities: ["long_context", "structured_output", "function_calling", "retrieval_integration"]
  known_limitations:
    vendor_disclosed: |
      Visual understanding limited to classification and captioning; not robust for OCR or fine-grained reasoning.  
      Occasional factual drift and truncation in long-context reasoning.
    common_failure_modes: |
      Overconfidence in chain-of-thought reasoning; misinterpretation of ambiguous visual cues.
    unsuitable_use_cases: |
      Safety-critical or forensic image analysis; real-time decision automation.

# # =============================================================================
# # [TRAINING & DATA]
# # Information on the data and methodology used to train the model.
# # =============================================================================
training_information:
  training_data_description: |
    Trained on extensive text and image-caption datasets from public, licensed, and synthetic sources.  
    Includes diverse global data with multilingual coverage.
  training_methodology: |
    Reinforcement Learning from Human Feedback (RLHF) and constitutional fine-tuning with large-context optimization.  
    Enhanced inference scaling for batch and streaming operations.
  data_privacy_considerations: |
    OpenAI excludes API data from training unless explicitly opted in by the user or organization.

# # =============================================================================
# # [INTENDED USE & SCOPE]
# # Guidance on suitable, unsuitable, and out-of-scope applications.
# # =============================================================================
intended_use:
  vendor_intended_use: |
    Enterprise, developer, and research users requiring high reasoning capacity at lower cost than GPT-4 baseline.  
    Ideal for documentation generation, code completion, and structured reasoning.
  suitable_domains: ["enterprise_QA", "software_engineering", "content_generation", "document_analysis"]
  out_of_scope_use: |
    Autonomous agents or safety-critical systems requiring deterministic outputs.

# # =============================================================================
# # [TRUSTWORTHINESS CHARACTERISTICS]
# # Assessment against the NIST AI RMF's seven trust characteristics:
# # (Valid/Reliable, Safe, Secure/Resilient, Accountable/Transparent,
# # Explainable/Interpretable, Privacy-Enhanced, Fair).
# # =============================================================================
trustworthiness_assessment:
  valid_and_reliable:
    vendor_claims: |
      GPT-4 Turbo offers GPT-4-level reasoning accuracy with significantly lower cost.  
      Optimized for reproducibility and stability in API usage.
    public_evidence: |
      Independent evaluations confirm benchmark parity with Claude 3 Sonnet and Gemini 1.5 Pro.
    assessment_notes: |
      Reliable for most reasoning tasks; latency and transparency improvements noted.
  safe:
    safety_measures: |
      RLHF alignment; content filters; enterprise-grade moderation API integration.  
    known_safety_issues: |
      False-positive refusals in sensitive text generation; residual hallucination in creative synthesis.
    assessment_notes: |
      Safe for enterprise usage with proper moderation.
  secure_and_resilient:
    security_features: |
      SOC 2 and ISO 27001 compliance; dedicated enterprise data isolation.  
      Supports private deployments via Azure OpenAI Service.
    known_vulnerabilities: |
      Standard prompt injection and jailbreak risks; mitigated by moderation pipeline.
    assessment_notes: |
      Secure for regulated enterprise use under documented guardrails.
  accountable_and_transparent:
    transparency_level: "Medium"
    auditability: |
      Public system card available; internal architecture details undisclosed.  
    assessment_notes: |
      Moderate transparency; full auditability through API logging.
  explainable_and_interpretable:
    explainability_features: |
      Deterministic JSON mode; consistent reasoning summaries; tool call traceability.
    interpretability_limitations: |
      Internal reasoning inaccessible; multimodal embeddings opaque.  
    assessment_notes: |
      Functionally interpretable for enterprise applications.
  privacy_enhanced:
    privacy_features: |
      No training on customer data; encryption at rest and in transit; optional regional data residency.
    privacy_concerns: |
      Limited control over inference-time telemetry in free-tier usage.  
    assessment_notes: |
      Meets enterprise data privacy expectations.
  fair_with_harmful_bias_managed:
    bias_mitigation: |
      Continuous fairness and bias auditing; red-teaming for cultural and gendered content.
    known_biases: |
      Subtle language bias in cultural analogies and occupational stereotypes.  
    assessment_notes: |
      Fairness well-controlled for enterprise-grade text generation.

# # =============================================================================
# # [EVALUATION GUIDANCE]
# # Recommendations for testing, validating, and comparing the model.
# # =============================================================================
evaluation_guidance:
  recommended_tests: |
    - Long-context recall and factual consistency validation  
    - Reasoning and chain-of-thought correctness measurement  
    - Bias and fairness evaluation for regulated contexts  
    - Latency and cost-performance benchmarking
  key_evaluation_questions: |
    - Is model accuracy sufficient for high-stakes reasoning?  
    - Does long-context handling meet expected precision?  
    - Are moderation and data policies aligned with compliance standards?
  comparison_considerations: |
    - Roughly equal reasoning strength to Claude 3 Sonnet and Gemini 1.5 Pro;  
      weaker multimodality than GPT-4o but more cost-efficient;  
      faster and cheaper than GPT-4 (2023 baseline).

# # =============================================================================
# # [NIST AI RMF MAPPING]
# # Mapping of model risks to the RMF lifecycle functions (Govern, Map,
# # Measure, Manage).
# # =============================================================================
rmf_function_mapping:
  govern:
    notes: |
      Document use policy and compliance scope for GPT-4 Turbo API integrations.
  map:
    context_considerations: |
      Identify cost, latency, and governance constraints in enterprise usage.  
    risk_categories: ["hallucination", "bias", "prompt_injection", "privacy_leakage"]
  measure:
    suggested_metrics: |
      Factual accuracy, hallucination rate, latency per 1K tokens, cost-efficiency ratio.  
  manage:
    risk_management_considerations: |
      Integrate moderation APIs; apply model version pinning for traceability.

# # =============================================================================
# # [REFERENCES & SOURCES]
# # Citations and links to source documentation, papers, and articles.
# # =============================================================================
references:
  vendor_documentation:
  - url: "https://openai.com/index/gpt-4-turbo/"
    description: "Official GPT-4 Turbo announcement"
  - url: "https://platform.openai.com/docs/models/gpt-4-turbo"
    description: "OpenAI model documentation and usage guide"
  benchmarks:
  - name: "MMLU"
    url: "https://platform.openai.com/docs/models/gpt-4-turbo"
    result: "87.5"
  - name: "GSM8K"
    url: "https://platform.openai.com/docs/models/gpt-4-turbo"
    result: "93.8"
  third_party_evaluations:
  - source: "ARC Benchmark Consortium (2024)"
    url: "https://arxiv.org/abs/2406.01864"
    summary: "Validated GPT-4 Turbo’s reasoning parity with Claude 3 Sonnet and Gemini 1.5 Pro."
  news_coverage:
  - title: "OpenAI releases GPT-4 Turbo — cheaper, faster, and longer context"
    url: "https://openai.com/index/gpt-4-turbo/"
    date: "2024-04-09"

# # =============================================================================
# # [CARD METADATA]
# # Information about the model card document itself (author, version, history).
# # =============================================================================
metadata:
  card_version: "1.0"
  card_author: "NIST AI RMF Cookbook Team"
  card_creation_date: "2025-10-17"
  last_updated: "2025-10-17"
  information_sources: |
    OpenAI GPT-4 Turbo release documentation, API system card, and benchmark data.
  completeness_assessment: |
    High for reasoning and performance transparency; medium for multimodal and dataset detail.  
  change_log:
  - date: "2025-10-17"
    author: "Cookbook Team"
    changes: "Initial card created from GPT-4 Turbo April 2024 release materials."
